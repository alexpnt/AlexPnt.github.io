<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Best books read in 2021</title>
      <link href="/2022/04/17/best-books-of-2021/"/>
      <url>/2022/04/17/best-books-of-2021/</url>
      
        <content type="html"><![CDATA[<h2 id="reading-during-2021">Reading during 2021</h2><p>Another year has passed and as usual, I recollect the best books I have read during 2021. Reading is always a good opportunity to learn something new and to become a bit wiser by reading the stories (fictional or not), ideas, thoughts and experiences conveyed by other people through words.</p><p>This post shows a list of the best books that I enjoyed most during 2021, together with a short review.</p><h2 id="foundation">Foundation</h2><p><img src="/images/best-books-of-2021/foundation.jpg" /></p><p>"Foundation", by Isaac Asimov, is set in a universe where a galactic empire has been successfully established for over twelve thousand years. However, even in its apparent robustness and mighty economic power, a mathematician named Hari Seldon hypothesizes and predicts its fall. This crumble, which is already in motion when Hari makes the first warning, will turn the empire into a decadent place to which a period of dark ages, barbarism, and anarchism of about thirteen thousand years will follow before a new empire can be reborn from its ashes. However, Hari not only foresees the collapse, but also proposes a solution to diminish the extreme consequences, that if applied successfully will shorten the dark period to only a one thousand years. The suggested solution involves the creation of two groups of scientists and engineers that together will build, guide, and influence the future events needed to make the second empire a reality faster than the predicted thirteen thousand years.</p><p>The book is very easy to read, as this was originally a collection of short stories that were joint together later. The story itself moves incredibly fast and thus the reader never gets bored. We read about all sorts of political, scientific, and economical events that occurred between the last years of the empire and the first phases critical to the creation of a new second empire. We also get to know more about the science behind Hari predictions, psychohistory, and how it combines statistical methods that work on large sample sizes and sociology.</p><p>It is not the best book of the original trilogy, but it is still very good since it setups the initial plot and paves the way for the next big events that will be described in the next books.</p><h2 id="foundation-and-empire">Foundation and Empire</h2><p><img src="/images/best-books-of-2021/foundation-and-empire.jpg" /></p><p>"Foundation and Empire", by Isaac Asimov, continues to develop the story of the Foundation, many years after the events described in the first book. The book is divided into two main parts: The General and The Mule. In the first part, we see how the Foundation faced a new Seldon crisis, how it dealt with a general, named Bel Riose, who was quickly gaining power and popularity, and once again test if the predictions of Hari Seldon still hold regarding the success of the Foundation. In the second part, we are again fast-forwarded many years into the future where the Foundation is greatly developed and has expanded itself across the galaxy and controls a new economic and prosperous system which is a potential successor to the old empire. We are also introduced to some new characters, such as Toran Darell, Bayta Darell, the psychologist Ebling Mis, and the mysterious character called "The Mule", a man who was born with unique abilities that allow him to manipulate the emotional state of those around him and wishes to completely dominate and rule the existing Foundation and every planet that is left of the empire, including the destruction of the rumored Second Foundation.</p><p>The story moves very fast and it is in general easy to follow. The second part is better than the first and introduces the reader to a major player, which is likely the biggest challenge the Foundation ever faced since even Seldon's predictions failed to account for it. We also get to know a little more about the potential existence of a hidden and mysterious Second Foundation. The last part is a setup for the events that happen in the last book of the original trilogy.</p><h2 id="second-foundation">Second Foundation</h2><p><img src="/images/best-books-of-2021/second-foundation.jpg" /></p><p>"Second Foundation", by Isaac Asimov, the last volume of the initial trilogy, focuses on the search of the rumored Second Foundation, a group of non-conventional scientists that had advanced knowledge of the way our mind and emotions work and were trained to manipulate them as well.</p><p>We are presented by two attempts to find it, either by "The Mule", the mysterious mutant with emotional control abilities that ruled what was once known as the Foundation system, and also the search by the scientists of the Foundation itself, since even them were blocked from the whereabouts and the true nature of the Second Foundation and only hinted at its existence. Its purpose was to ensure the success of Hari Seldon's plan by using the First Foundation to carry it out. All this control was performed from afar and with cirurgical interventions without the Foundation scientists being aware of such actions. The reason for this is well explained and explored in the book.</p><p>As usual, the book moves quickly through the main story. This is arguably the best book of the original trilogy and culminates in a big reveal about the very nature and location of the Second Foundation. The book theme is human emotions and advanced mind control or psychology which is a big contrast against the hard sciences and physics that we are usually trained on since early age. It explores the hypothetical situation where psychology and the science of the mind were further explored, developed, and understood at the same level we understand physics from our material world, which makes it a great for fans of psychological sciences as well.</p><h2 id="factfulness">Factfulness</h2><p><img src="/images/best-books-of-2021/factfulness.jpg" /></p><p>Factulness, by Hans Rosling, is a book that challenges our current view of the world. It is about those kinds of facts that we think we are well aware of but it is very likely that we are completely wrong. After we are led to answer a simple and straightforward questionnaire about the current state of many issues such as education, health, demographics, environment, or wealth around the world, we are presented with the actual truth, based on real and updated data, regarding these matters and how it differs (on the majority of cases) to our own awareness, independently of where we are located in the world or our level of education. It shows how there is a high chance that we have an outdated view of the world. Not only that, but it also explores the potential causes that might be responsible for this poor awareness of what is going on around the globe.</p><p>The rest of the book focuses on educating and equipping ourselves with 10 tools that allow us to identify misconceptions in the information that is shown to us and that we are led to believe or to infer the wrong conclusions. The major message is that some issues today might be considered to be in a bad state, but they have been in constant improvement across history and are actually better than they were decades ago.</p><p>It is a very good book that gives the reader a new perspective regarding the current state of the world, and leaves a positive tone for the future of humanity, while not ignoring issues that still need improvement.</p><h2 id="inteligência-artificial-artificial-intelligence">Inteligência Artificial (Artificial Intelligence)</h2><p>Inteligência Artificial ("Artificial Intelligence" in English) written by Arlindo Oliveira is a brief history of the evolution of humanity over time. The book follows the timeline from the first moments after the Big Bang until the appearance of the first signs of life, including a brief reference to the underlying biological and chemical processes. It also explores the evolution from the first intelligent beings to the human form we know today.</p><p>Before the topic of intelligence is covered, there is a brief history of the various technologies that have been created over the centuries, notably those that manipulated materials and food to improve the quality of life. The developments of writing and mathematics are also covered, as they are the main technologies that allowed humanity to make great advances over the generations, as these allowed knowledge and culture to no longer be limited to the short time window of a human life.</p><p>The various industrial revolutions brought about by the emergence of these new technologies are addressed, with special emphasis on the latest one that makes use of digital systems to transmit and manipulate information, culminating in the central question of whether these systems can become intelligent. A historical perspective is presented on the first developments in this area, such as the differential mechanical systems of Charles Babbage, referencing his analytical engine that was later studied by Ada Lovelace and finally the thesis of Turing and Church where the possibility of the existence of artificial intelligence is questioned and several objections are presented. After these theoretical advances are discussed, the phases of great progress are presented where systems that tried to simulate intelligent behaviour through the manipulation of symbols, as well as natural language processing systems, were developed, although with some periods of stagnation. Modern techniques for developing intelligent systems are also mentioned, namely approaches that rely on learning by experience and observations of the results of a system where intelligent behaviour is inferred rather than explicitly defined. As examples of these approaches, statistical methods and neural networks inspired by the biological functioning of neurons are mentioned. Both methods are used frequently nowadays.</p><p>Finally, the book offers some visions of what the future might look like, namely how these intelligent systems might one day be created and in what form. However, several objections are raised that may hinder or even make this goal impossible, such as physical limitations like the extreme complexity of emulating all the electrical and biochemical processes of a brain in a digital system, the required computational power or the fact that the brain is not independent from the environment and system (body) in which it is inserted. Ethical issues where a hypothetical digital brain ceases to be a mere program or the old question about whether it could possess consciousness or not are also addressed. In addition to these questions it also offers some reflections on the transformation at the level of society that will occur, such as the disappearance of jobs that have become obsolete, the creation of new jobs or the challenges of redistribution of wealth associated with the increasing automation.</p><p>It is an easy and quick read, and exposes a number of intriguing social and ethical questions about the future of intelligent systems.</p><p><img src="/images/best-books-of-2021/ia.jpg" /></p><h2 id="robotic-process-automation-and-risk-mitigation">Robotic Process Automation and Risk Mitigation</h2><p><img src="/images/best-books-of-2021/rpa.jpg" /></p><p>Robotic Process Automation and Risk Mitigation written by Mary C. Lacity and Leslie P. Willcocks is a business guide on how to make the shift from traditional processes to the new era of automation.</p><p>It focuses on the automation of tasks that can be entirely replaced, leaving the humans dealing with exceptions or more complex situations. It shows how this robotic automation can increase the value of shareholders, customers, and employees contrary to the popular fears of mass unemployment and crisis that usually follow big disruptions like these and presents a risk mitigation framework that can be followed in order to handle potential outcomes that may arise during the implementation of automation processes. It also addresses the redesign of current roles in order to adapt them for the future.</p><p>It is a quick read and a good reference.</p><h2 id="economics-101">Economics 101</h2><p><img src="/images/best-books-of-2021/economics-101.jpg" /></p><p>Economics 101, from Alfred Mill, is a nice introduction to Economics. It is a quick reference book that presents and explains many important concepts needed to better understand the many economic phenomenons that we are constantly witnessing around the world. It is heavily focused on a US perspective, but the theory can be applied everywhere.</p><p>It is a concise and quick read. Sometimes, the explained concepts are only superficially explored and the author does not go further than that, which is reasonable for an introductory book. This is a good book for those who want to know a bit more about the basic fundamentals that govern the economic world.</p>]]></content>
      
      
      
        <tags>
            
            <tag> reading </tag>
            
            <tag> books </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Best books read in 2020</title>
      <link href="/2021/01/14/best-books-of-2020/"/>
      <url>/2021/01/14/best-books-of-2020/</url>
      
        <content type="html"><![CDATA[<h2 id="on-reading-and-books">On reading and books</h2><p>Books have immense value for what they actually cost. As Carl Sagan once said, "Books are key to understanding the world" and, for that matter, reading is a very rewarding activity. Bellow is an unordered list of the best books, along with a small summary, that I read during 2020.</p><h2 id="dune">Dune</h2><p><img src="/images/best-books-of-2020/dune.jpeg" /></p><p>"Dune", by Frank Herbert is one of those foundational books that inspired many future works. It tells the journey of Paul, from his young years in the planet Caladan until it leads a revolution in the desert planet of Arrakis, also known as Dune.</p><p>It's a story full of great battles, betrayals, political drama, power games, and scientific dialogues. It also contains philosophical wisdom about time, free will, artificial intelligence, and human nature.</p><p>The information density can be high at times, which requires constant motivation to go on. However, the story gets better and better as we read it.</p><p>It is a very good work of fiction, and the first glimpse of an entire universe full of awesome and intriguing details.</p><h2 id="the-art-of-war">The Art of War</h2><p><img src="/images/best-books-of-2020/art-of-war.jpeg" /></p><p>"The Art of War", written by Sun Tzu is a timeless work with many pieces of wisdom and philosophical insights.</p><p>This is a book to read many times and probably at different points in our lives. It should be seen as a reference book with strategies that we can follow according to our current circumstances, and it is adequate for anyone who finds himself faced with difficult decisions or just wants to know what move should be taken next.</p><p>The title can be misleading at first since it was thought of in the context of war. However, its general principles can be used in many aspects of our lives.</p><p>The book is small, easy to follow, but full of reflections. A generally good read that will leave its readers thinking about it.</p><h2 id="fahrenheit-451">Fahrenheit 451</h2><p><img src="/images/best-books-of-2020/f451.jpeg" /></p><p>"Fahrenheit 451", by Ray Bradbury, tells the story of a man, Guy Montag, whose job is to burn forbidden weapons of knowledge and education such as books. It happens in a future world where the fundamental meaning of the word "fireman" has been changed. It no longer means stopping fires but rather secretly police anyone who is suspected of owning forbidden publications and consequently proceed to erase any trace of their existence.</p><p>Similar to other dystopian novels, the protagonist reflects on his doings and finds himself rebelling against the established system, where independent and original ideas are no longer accepted. Instead, mass controlling of the way people think, and the constant degradation of old values are the norm.</p><p>This work is one of the best, concise, and accessible dystopian novels and a good critic of how mass media is overthrowing the interest in books by pushing empty content with degraded value.</p><h2 id="animal-farm">Animal Farm</h2><p><img src="/images/best-books-of-2020/animal-farm.jpeg" /></p><p>"Animal Farm", by George Orwell is yet another brilliant work that warns us of the dangers of corruption gained through excessive power.</p><p>Throughout the work, we observe the slow rise of the pig Napoleon as he continually gains more power and control of the farm and its population. The animals watch in despair to what once was a utopian farm ruled by animals on fair values turning into a harmful place full of lies, false propaganda, inequality, and oppression.</p><p>This is a small story with a powerful message.</p><h2 id="we">We</h2><p><img src="/images/best-books-of-2020/we.jpeg" /></p><p>"We", by Yevgeny Zamyatin, transports us to a futuristic world made of transparent glass where individualism, privacy, the "I", is abolished. Only the mindset aligned with the collective social body, "We", is allowed and any deviation from this norm of thinking is considered an illness. Readers familiar with the works of George Orwell (1984) and Aldous Huxley (Brave New World) will find many similarities and regard this work as a source of inspiration.</p><p>We learn about this world through what is left of the personal diary of our protagonist, D-503, a loyal chief engineer and a mathematician, responsible for the construction of a space shuttle that will colonize planets and spread the ideology. D-503 works for the totalitarian regime of One state, where happiness was supposedly achieved, albeit at a great cost. The diary presented to us shows the reflections of a man bounded by the perfect, rigorous, and harmonic beauty of mathematics and how it governs the world. It also gives us a glimpse of the struggles that D-503 faces when confronted with subjects considered obsolete, such as art, creativity, imagination, and ultimately love, in which the rebel female character I-330 plays a striking role.</p><p>At times, it seems like the records of a sane man descending into madness, where madness in this case is what one would call sanity in a truly free world.</p><p>The writing is straightforward, fast-paced, and concise and it can be seen as a foundational prototype for the dystopian genre that gave rise to many other great works. Comparing with 1984 or Brave New World, this work is much easier to digest and consume and an adequate introduction to the genre.</p><p>It is a good story, and a must-read for anyone interested in political dystopias.</p><h2 id="section">1984</h2><p><img src="/images/best-books-of-2020/1984.jpeg" /></p><p>"1984", by George Orwell is a dystopian novel about a very dark future where individual liberties belong to the past and are no longer permitted. Through permanent surveillance and constant manipulation of facts with alternative versions that suit the good image of the IngSoc party, the collective social body is controlled, manipulated, brainwashed, and transformed into obedient pawns. Even thoughts are controlled to an extent by using the newly created language of Newspeak, a very concise language that restricts freedom of thought and that will slowly replace the old English standard.</p><p>The story revolves around Winston Smith, who works in the "Ministry of Truth" department where truth is the subject constantly exposed to manipulation and where history is rewritten. Frustrated with the system, Winston rebels against the totalitarian party of Big Brother and keeps a diary of his inner thoughts, therefore committing a crime. Winston, along with his secret lover Julia, make plans throughout the story to escape the control of the party and achieve the desired freedom. The ending of 1984 is a moment worth remembering.</p><p>Readers knowledgeable about the work of Yevgeny Zamyatin, namely "We" will find the idea of keeping a private record of events and self-reflections in a heavily surveilled scenario, a familiar idea.</p><p>This work is a warning to humanity to avoid giving too much control and power to a few entities that can easily turn a free land into a place without freedom or justice where everything that people do, say, and speak is scrutinized.</p><p>It is also a heavily political lesson. By reading this book readers will learn more about politics, namely about socialism and its dangerous radical versions even if was not their original intent.</p><h2 id="brave-new-world">Brave New World</h2><p><img src="/images/best-books-of-2020/brave-new-world.jpeg" /></p><p>"Brave new world", by Aldous Huxley, is a futuristic dystopia, happening in a highly developed society, where old institutions are dead and replaced by opposite forms that do not pose any danger to the pillars of stability. Dangerous values such as individualism, books that encourage independent thinking, family, monogamy, and religions are abolished and very disliked. Birth is also abolished. People are not born anymore. They are engineered.</p><p>In this heavily hierarchized society composed of 5 castes, everyone belongs to everyone else, with the exception that one cannot get involved with another person from a different class. Every caste is composed of people with specific characteristics, skills, purpose, and tasks, ranging from the most intellectual alphas and betas to the almost dehumanized, mass-produced delta and epsilon workers.</p><p>This controlled nature is one of the fundamental properties that ensures the stability of the society since everyone is genetically engineered to have the characteristics and attributes from their assigned caste. Besides this engineered predeterminism, newborns suffer a long brainwashing process from childhood until adulthood in order to be indoctrinated and conditioned into pursuing a set of beliefs that keep the overall society intact, such as hate for nature and books and love for expensive hobbies and activities that support the economy. Nevertheless, for every personal problem, there is a solution, such as the Soma drug, the free drug that brings instant happiness, without side effects.</p><p>The old way of life belongs to the past and can only be seen in some reservations where people, called "savages", still live according to the old customs.</p><p>In a world where there is only one god, Ford, the original developer of the assembly line, nothing seems to be able to shake the foundations of this system, except maybe for our protagonist, John, "the Savage", that comes from the savage lands, and is probably the only human being coming to the World State, that remains in its original and purest form and can truly see the Brave New World society through a different prism for what it really is. Together with other citizens of the civilization, namely Bernard and Helmholtz, two unhappy alphas that wish to have individuality, they represent the case against the established norm.</p><p>This is a striking view of a potential future. Some elements of this society may be already familiar in some places of today's earth. It is a very good book that gives yet another warning about the twisted and pseudo happy society that extremely forced group thinking can lead to. This "perfect" society is only perfect because its members are not allowed to think differently. Therefore, they are pseudo-happy but without freedom.</p><p>This book along with "We", by Yevgeny Zamyatin and "1984", by George Orwell, make the triad of political dystopias worth reading and easily capacitates one to identify the dangerous behaviors that can have bad consequences.</p><h2 id="final-remarks">Final remarks</h2><p>Most of these books served as prototypes for other future works and were very important when they were written since they critiqued the society but were disguised as works of fiction. Nevertheless, they have important messages that should not be ignored.</p>]]></content>
      
      
      
        <tags>
            
            <tag> reading </tag>
            
            <tag> books </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Speeding up python programs with Numba and Numpy</title>
      <link href="/2018/10/19/speeding-up-python/"/>
      <url>/2018/10/19/speeding-up-python/</url>
      
        <content type="html"><![CDATA[<h2 id="when-python-is-not-enough">When Python is not enough</h2><p>The Python programming language is a great tool for almost any kind of rapid prototyping and quick development. It has great features such as its high level nature, a syntax with almost human-level readability . Besides, it is cross platform, with a a diverse standard library and it is multi-paradigm, giving a lot of freedom to the programmer which can use different programming paradigms such as <a href="https://en.wikipedia.org/wiki/Object-oriented_programming">object-oriented</a>, <a href="https://en.wikipedia.org/wiki/Functional_programming">functional</a> or <a href="https://en.wikipedia.org/wiki/Procedural_programming">procedural</a> as he sees fit. However, sometimes some portion of our system has high performance requirements and thus the speed that Python offers might not be sufficient. So, how can we boost performance without leaving the realm of Python (using for example compiled languages such as C/C++ or JIT/compiled such as JAVA) and when all of our optimizations are not enough anymore ?</p><p>A possible solution, among others, is to make use of Numba, a runtime compiler that translates Python code to native instructions, while letting us use the concise and expressiveness power of Python and also achieve native code speed.</p><h2 id="whats-is-numba">Whats is Numba ?</h2><p><a href="https://numba.pydata.org/">Numba</a> is a library that performs <a href="https://en.wikipedia.org/wiki/Just-in-time_compilation">JIT</a> compilation, that is, translates pure python code to optimized machine code at runtime, using the <a href="https://llvm.org/">LLVM</a> industry-standard compiler. It is also able to automatically parallelize loops and run them on multiple cores. Numba is cross-platform since it works on different operative systems (Linux, Windows, OSX) and different architectures (x86, x86_64, ppc64le, etc). It is also able to run the same code on a GPU (NVIDIA CUDA or AMD ROC) and is compatible with Python 2.7 and 3.4-3.7. Overall, the most impressive feature is its simplicity of use since we only need a few decorators to leverage the full power of JIT optimizations.</p><h2 id="numba-modes-and-the-jit-decorator">Numba modes and the <span class="citation" data-cites="jit">@jit</span> decorator</h2><p>The most important instruction is the <em><span class="citation" data-cites="jit">@jit</span></em> decorator. It is this decorator that instructs the compiler which mode to run and with what configurations . Under the hood, the generated bytecode of our decorated functions combined with the arguments that we specify in the decorator, such as the type of the input arguments, are analysed, optimized and finally compiled with the LLVM, generating specially tailored native machine instructions for the CPU currently in use. This compiled version is then reused for each function call.</p><p>There are two important modes: <em>nopython</em> and <em>object</em>. The <em>nopython</em> completely avoids the python interpreter and translates the full code to native instructions that can be run without the help of Python . However, if for some reason, that mode is not available (for example, when using unsupported Python features or external libraries) the compilation will fall back to the <em>object</em> mode, where it uses the Python interpreter when it is unable to compile some code . Naturally, the <em>nopython</em> mode is the one who offers the best performance gains.</p><h2 id="high-level-architecture-of-numba">High-level architecture of Numba</h2><p>The Numba translation process can be translated in a set of important steps ranging from the Bytecode analysis to the final machine code generation. The picture bellow illustrates this process, where the green boxes correspond to the frontend of the Numba compiler and the blue boxes belong to the backend.</p><p><img src="/images/numba/numba-arch.png" /></p><p>The Numba compiler starts by doing an extensive analysis on the byecode of the desired function(s). This step produces a graph describing the possible flow of executions, called control flow graph (CFG). Based on this graph an analysis on the variable lifetimes are calculated. With these steps finished, the compiler starts translating the bytecode into an intermediate representation (IR) where Numba will perform further optimizations and transformations. Afterwards, type inference, one of the most important steps, is performed. In this step, the compiler will try to infer the type of all variables. Furthermore, if the parallel setting is enabled, the IR code will be transformed to an equivalent parallel version.</p><p>If all types are successfully inferred, the Numba IR code is then translated into an efficient LLVM IR code, completely avoiding the Python runtime. However, if the type inference process fails, the LLVM generated code will be slower since it still needs to deal with calls to Python C API. Finally, the LLVM IR code is compiled into native instructions by the LLVM JIT compiler. This optimized machined code is then loaded into memory and reused across multiple calls to the same function(s), making it hundreds of times faster than pure Python.</p><p>For debugging purposes, Numba also offers a set of flags that can be enabled in order to see the generated output of the different stages.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">os.environ[<span class="string">&quot;NUMBA_DUMP_CFG&quot;</span>] = <span class="string">&quot;1&quot;</span></span><br><span class="line">os.environ[<span class="string">&quot;NUMBA_DUMP_IR&quot;</span>] = <span class="string">&quot;1&quot;</span></span><br><span class="line">os.environ[<span class="string">&quot;NUMBA_DUMP_ANNOTATION&quot;</span>] = <span class="string">&quot;1&quot;</span></span><br><span class="line">os.environ[<span class="string">&quot;NUMBA_DEBUG_ARRAY_OPT_STATS&quot;</span>] = <span class="string">&quot;1&quot;</span></span><br><span class="line">os.environ[<span class="string">&quot;NUMBA_DUMP_LLVM&quot;</span>] = <span class="string">&quot;1&quot;</span></span><br><span class="line">os.environ[<span class="string">&quot;NUMBA_DUMP_OPTIMIZED&quot;</span>] = <span class="string">&quot;1&quot;</span></span><br><span class="line">os.environ[<span class="string">&quot;NUMBA_DUMP_ASSEMBLY&quot;</span>] = <span class="string">&quot;1&quot;</span></span><br></pre></td></tr></table></figure><h2 id="speeding-numerical-computations-an-example">Speeding numerical computations: An example</h2><p>The best use case where we can make use of the Numba library is when we have to do intensive numerical computations. As an example, let's compute the softmax function on a set of 2^16 (65536) random numbers. The softmax function, useful to convert a set of real values into probabilities and commonly used as the last layer in neural networks architectures, is defined as:</p><p><span class="math display">\[ \sigma(z_j) = { e^{z_j}  \over \sum_{k=1}^{K}  e^{z_k} } \]</span></p><p>The following code show two different implementations of this function, a pure python approach (<em>softmax_python</em>) and an optimized version making using of numpy and numba (<em>softmax_optimized</em>):</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">import time</span><br><span class="line">import math</span><br><span class="line">import numpy as np</span><br><span class="line">from numba import jit</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">@jit(<span class="string">&quot;f8(f8[:])&quot;</span>, cache=False, nopython=True, nogil=True, parallel=True)</span><br><span class="line">def esum(z):</span><br><span class="line">    <span class="built_in">return</span> np.sum(np.exp(z))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">@jit(<span class="string">&quot;f8[:](f8[:])&quot;</span>, cache=False, nopython=True, nogil=True, parallel=True)</span><br><span class="line">def softmax_optimized(z):</span><br><span class="line">    num = np.exp(z)</span><br><span class="line">    s = num / esum(z)</span><br><span class="line">    <span class="built_in">return</span> s</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def softmax_python(z):</span><br><span class="line">    s = []</span><br><span class="line"></span><br><span class="line">    exp_sum = 0</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(z)):</span><br><span class="line">        exp_sum += math.exp(z[i])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(z)):</span><br><span class="line">        s += [math.exp(z[i]) / exp_sum]</span><br><span class="line"></span><br><span class="line">    <span class="built_in">return</span> s</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line">    np.random.seed(0)</span><br><span class="line">    z = np.random.uniform(0, 10, 10 ** 8)   <span class="comment"># generate random floats in the range [0,10)</span></span><br><span class="line"></span><br><span class="line">    start = time.time()</span><br><span class="line">    softmax_python(z.tolist())          <span class="comment"># run pure python version of softmax</span></span><br><span class="line">    elapsed = time.time() - start</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Ran pure python softmax calculations in &#123;&#125; seconds&#x27;</span>.format(elapsed))</span><br><span class="line"></span><br><span class="line">    softmax_optimized(z)                     <span class="comment"># cache jit compilation</span></span><br><span class="line">    start = time.time()</span><br><span class="line">    softmax_optimized(z)                     <span class="comment"># run optimzed version of softmax</span></span><br><span class="line">    elapsed = time.time() - start</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;\nRan optimized softmax calculations in &#123;&#125; seconds&#x27;</span>.format(elapsed))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><p>The result of the above script will be something similar to:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt; python softmax.py</span><br><span class="line"></span><br><span class="line">Ran pure python softmax calculations <span class="keyword">in</span> 41.62234306335449 seconds</span><br><span class="line"></span><br><span class="line">Ran optimized softmax calculations <span class="keyword">in</span> 1.7453773021697998 seconds</span><br></pre></td></tr></table></figure><p>These results clearly shows the performance gains obtained when converting our code to something that Numba understands well.</p><p>In the <em>softmax_optimized</em> function, there is already the Numba annotation that leverages the full power of JIT optimizations. In fact, under the hood the following bytecode will be analysed, optimized and compiled to native instructions:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">&gt; python</span><br><span class="line"><span class="keyword">import</span> dis</span><br><span class="line"><span class="keyword">from</span> softmax <span class="keyword">import</span> esum, softmax_optimized</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>dis.dis(softmax_optimized)</span><br><span class="line"> <span class="number">14</span>           <span class="number">0</span> LOAD_GLOBAL              <span class="number">0</span> (np)</span><br><span class="line">              <span class="number">2</span> LOAD_ATTR                <span class="number">1</span> (exp)</span><br><span class="line">              <span class="number">4</span> LOAD_FAST                <span class="number">0</span> (z)</span><br><span class="line">              <span class="number">6</span> CALL_FUNCTION            <span class="number">1</span></span><br><span class="line">              <span class="number">8</span> STORE_FAST               <span class="number">1</span> (num)</span><br><span class="line"></span><br><span class="line"> <span class="number">15</span>          <span class="number">10</span> LOAD_FAST                <span class="number">1</span> (num)</span><br><span class="line">             <span class="number">12</span> LOAD_GLOBAL              <span class="number">2</span> (esum)</span><br><span class="line">             <span class="number">14</span> LOAD_FAST                <span class="number">0</span> (z)</span><br><span class="line">             <span class="number">16</span> CALL_FUNCTION            <span class="number">1</span></span><br><span class="line">             <span class="number">18</span> BINARY_TRUE_DIVIDE</span><br><span class="line">             <span class="number">20</span> STORE_FAST               <span class="number">2</span> (s)</span><br><span class="line"></span><br><span class="line"> <span class="number">16</span>          <span class="number">22</span> LOAD_FAST                <span class="number">2</span> (s)</span><br><span class="line">             <span class="number">24</span> RETURN_VALUE</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>dis.dis(esum)</span><br><span class="line">  <span class="number">9</span>           <span class="number">0</span> LOAD_GLOBAL              <span class="number">0</span> (np)</span><br><span class="line">              <span class="number">2</span> LOAD_ATTR                <span class="number">1</span> (<span class="built_in">sum</span>)</span><br><span class="line">              <span class="number">4</span> LOAD_GLOBAL              <span class="number">0</span> (np)</span><br><span class="line">              <span class="number">6</span> LOAD_ATTR                <span class="number">2</span> (exp)</span><br><span class="line">              <span class="number">8</span> LOAD_FAST                <span class="number">0</span> (z)</span><br><span class="line">             <span class="number">10</span> CALL_FUNCTION            <span class="number">1</span></span><br><span class="line">             <span class="number">12</span> CALL_FUNCTION            <span class="number">1</span></span><br><span class="line">             <span class="number">14</span> RETURN_VALUE</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>We can provide more information about the expected input and output types through the signature. In the example above, the signature "<em>f8[:](f8[:])</em>" is used to specify that the function accepts an array of double precision floats and returns another array of 64-bit floats.</p><p>The signature could also use the explicit type names: "<em>float64[:](float64[:])</em>". The general form is <em>type(type, type, ...)</em> which resembles a typical function where argument names are replaced by their types and the function name is replaced by its return type. Numba accepts many different types, which are described bellow:</p><table><thead><tr class="header"><th style="text-align: center;">Type name(s)</th><th style="text-align: center;">Type short name</th><th style="text-align: center;">Description</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">boolean</td><td style="text-align: center;">b1</td><td style="text-align: center;">represented as a byte</td></tr><tr class="even"><td style="text-align: center;">uint8, byte</td><td style="text-align: center;">u1</td><td style="text-align: center;">8-bit unsigned byte</td></tr><tr class="odd"><td style="text-align: center;">uint16</td><td style="text-align: center;">u2</td><td style="text-align: center;">16-bit unsigned integer</td></tr><tr class="even"><td style="text-align: center;">uint32</td><td style="text-align: center;">u4</td><td style="text-align: center;">32-bit unsigned integer</td></tr><tr class="odd"><td style="text-align: center;">uint64</td><td style="text-align: center;">u8</td><td style="text-align: center;">64-bit unsigned integer</td></tr><tr class="even"><td style="text-align: center;">int8, char</td><td style="text-align: center;">i1</td><td style="text-align: center;">8-bit signed byte</td></tr><tr class="odd"><td style="text-align: center;">int16</td><td style="text-align: center;">i2</td><td style="text-align: center;">16-bit signed integer</td></tr><tr class="even"><td style="text-align: center;">int32</td><td style="text-align: center;">i4</td><td style="text-align: center;">32-bit signed integer</td></tr><tr class="odd"><td style="text-align: center;">int64</td><td style="text-align: center;">i8</td><td style="text-align: center;">64-bit signed integer</td></tr><tr class="even"><td style="text-align: center;">intc</td><td style="text-align: center;">–</td><td style="text-align: center;">C</td></tr><tr class="odd"><td style="text-align: center;">uintc</td><td style="text-align: center;">–</td><td style="text-align: center;">C</td></tr><tr class="even"><td style="text-align: center;">intp</td><td style="text-align: center;">–</td><td style="text-align: center;">pointer-sized integer</td></tr><tr class="odd"><td style="text-align: center;">uintp</td><td style="text-align: center;">–</td><td style="text-align: center;">pointer-sized unsigned integer</td></tr><tr class="even"><td style="text-align: center;">float32</td><td style="text-align: center;">f4</td><td style="text-align: center;">single-precision floating-point number</td></tr><tr class="odd"><td style="text-align: center;">float64, double</td><td style="text-align: center;">f8</td><td style="text-align: center;">double-precision floating-point number</td></tr><tr class="even"><td style="text-align: center;">complex64</td><td style="text-align: center;">c8</td><td style="text-align: center;">single-precision complex number</td></tr><tr class="odd"><td style="text-align: center;">complex128</td><td style="text-align: center;">c16</td><td style="text-align: center;">double-precision complex number</td></tr></tbody></table><p>These annotations are easily extended to array forms using [:] , [:, :] or [:, :, :] for 1, 2 and 3 dimensions, respectively.</p><h2 id="final-words">Final words</h2><p>Python is a great tool. However, it has some limitations which can be surpassed with the right strategy, making it competitive, in terms of performance, to other compiled languages. Although this post focused on the Numba library, there are other good options (which deserve their own dedicated blog post). Numba also offers other great features not explored here such as integration with NVIDIA CUDA/AMD ROC GPUs, interfacing with C functions and Ahead-of-Time compilation (AOT). Finally, the example source code can be found <a href="https://github.com/alexpnt/numba-examples">here</a>.</p>]]></content>
      
      
      
        <tags>
            
            <tag> development </tag>
            
            <tag> python </tag>
            
            <tag> performance </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Setting up tensorflow to run on a nvidia gpu</title>
      <link href="/2018/01/27/nvidia-cuda-for-deep-learning/"/>
      <url>/2018/01/27/nvidia-cuda-for-deep-learning/</url>
      
        <content type="html"><![CDATA[<p>Nowadays, there are have massive amounts of structured and unstructured data and good hardware resources, which makes it ideal for resource-hungry algorithms, such as deep neural networks, that usually need huge amounts of data. Furthermore, libraries such as <a href="https://www.tensorflow.org">Tensorflow</a> are a great boost in the development and deploy of accurate models. While Tensorflow can run on a typical CPU, for the best performance and reduced training/inference time we may run it on a GPU. Libraries such as NVIDIA CUDA Deep Neural Network library (cuDNN) greatly optimize low-level computations, such as complex matrix operations and deliver very good performance speedups. Before we can use Tensorflow we need to setup the cuDNN library. This post demonstrates a common scenario where we setup an Amazon GPU instance (p3.2xlarge), running a Ubuntu 16.04 x86_64 distribution with a NVIDIA Tesla V100.</p><h2 id="install-the-cuda-toolkit">Install the CUDA Toolkit</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mkdir cuda</span><br><span class="line"><span class="built_in">cd</span> cuda</span><br><span class="line">wget http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/cuda-repo-ubuntu1604_9.0.176-1_amd64.deb</span><br><span class="line">sudo dpkg -i cuda-repo-ubuntu1604_9.0.176-1_amd64.deb</span><br><span class="line">sudo apt-key adv --fetch-keys http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/7fa2af80.pub</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install cuda-9-0</span><br></pre></td></tr></table></figure><h2 id="setup-the-environment-variables">Setup the environment variables</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> PATH=/usr/<span class="built_in">local</span>/cuda-9.0/bin<span class="variable">$&#123;PATH:+:<span class="variable">$&#123;PATH&#125;</span>&#125;</span></span><br><span class="line"><span class="built_in">export</span> LD_LIBRARY_PATH=/usr/<span class="built_in">local</span>/cuda-9.0/lib64<span class="variable">$&#123;LD_LIBRARY_PATH:+:<span class="variable">$&#123;LD_LIBRARY_PATH&#125;</span>&#125;</span></span><br><span class="line"><span class="built_in">export</span> CUDA_HOME=/usr/<span class="built_in">local</span>/cuda-9.0/</span><br></pre></td></tr></table></figure><h2 id="compile-samples-and-test-them">Compile samples and test them</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cp -r /usr/<span class="built_in">local</span>/cuda-9.0/samples <span class="variable">$HOME</span>/cuda</span><br><span class="line"><span class="built_in">cd</span> samples</span><br><span class="line">make clean &amp;&amp; make</span><br></pre></td></tr></table></figure><h2 id="check-installation-by-running-devicequery-and-bandwidthtest">Check installation by running <em>deviceQuery</em> and <em>bandwidthTest</em></h2><p>You should see an output similar to these ones:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">cd bin&#x2F;</span><br><span class="line">.&#x2F;deviceQuery</span><br><span class="line"></span><br><span class="line">.&#x2F;deviceQuery Starting...</span><br><span class="line"></span><br><span class="line"> CUDA Device Query (Runtime API) version (CUDART static linking)</span><br><span class="line"></span><br><span class="line">Detected 1 CUDA Capable device(s)</span><br><span class="line"></span><br><span class="line">Device 0: &quot;Tesla V100-SXM2-16GB&quot;</span><br><span class="line">  CUDA Driver Version &#x2F; Runtime Version          9.0 &#x2F; 9.0</span><br><span class="line">  CUDA Capability Major&#x2F;Minor version number:    7.0</span><br><span class="line">  Total amount of global memory:                 16160 MBytes (16945512448 bytes)</span><br><span class="line">  (80) Multiprocessors, ( 64) CUDA Cores&#x2F;MP:     5120 CUDA Cores</span><br><span class="line">  GPU Max Clock rate:                            1530 MHz (1.53 GHz)</span><br><span class="line">  Memory Clock rate:                             877 Mhz</span><br><span class="line">  Memory Bus Width:                              4096-bit</span><br><span class="line">  L2 Cache Size:                                 6291456 bytes</span><br><span class="line">  Maximum Texture Dimension Size (x,y,z)         1D&#x3D;(131072), 2D&#x3D;(131072, 65536), 3D&#x3D;(16384, 16384, 16384)</span><br><span class="line">  Maximum Layered 1D Texture Size, (num) layers  1D&#x3D;(32768), 2048 layers</span><br><span class="line">  Maximum Layered 2D Texture Size, (num) layers  2D&#x3D;(32768, 32768), 2048 layers</span><br><span class="line">  Total amount of constant memory:               65536 bytes</span><br><span class="line">  Total amount of shared memory per block:       49152 bytes</span><br><span class="line">  Total number of registers available per block: 65536</span><br><span class="line">  Warp size:                                     32</span><br><span class="line">  Maximum number of threads per multiprocessor:  2048</span><br><span class="line">  Maximum number of threads per block:           1024</span><br><span class="line">  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)</span><br><span class="line">  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)</span><br><span class="line">  Maximum memory pitch:                          2147483647 bytes</span><br><span class="line">  Texture alignment:                             512 bytes</span><br><span class="line">  Concurrent copy and kernel execution:          Yes with 2 copy engine(s)</span><br><span class="line">  Run time limit on kernels:                     No</span><br><span class="line">  Integrated GPU sharing Host Memory:            No</span><br><span class="line">  Support host page-locked memory mapping:       Yes</span><br><span class="line">  Alignment requirement for Surfaces:            Yes</span><br><span class="line">  Device has ECC support:                        Enabled</span><br><span class="line">  Device supports Unified Addressing (UVA):      Yes</span><br><span class="line">  Supports Cooperative Kernel Launch:            Yes</span><br><span class="line">  Supports MultiDevice Co-op Kernel Launch:      Yes</span><br><span class="line">  Device PCI Domain ID &#x2F; Bus ID &#x2F; location ID:   0 &#x2F; 0 &#x2F; 30</span><br><span class="line">  Compute Mode:</span><br><span class="line">     &lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt;</span><br><span class="line"></span><br><span class="line">deviceQuery, CUDA Driver &#x3D; CUDART, CUDA Driver Version &#x3D; 9.0, CUDA Runtime Version &#x3D; 9.0, NumDevs &#x3D; 1</span><br><span class="line">Result &#x3D; PASS</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;bandwidthTest</span><br><span class="line"></span><br><span class="line">[CUDA Bandwidth Test] - Starting...</span><br><span class="line">Running on...</span><br><span class="line"></span><br><span class="line"> Device 0: Tesla V100-SXM2-16GB</span><br><span class="line"> Quick Mode</span><br><span class="line"></span><br><span class="line"> Host to Device Bandwidth, 1 Device(s)</span><br><span class="line"> PINNED Memory Transfers</span><br><span class="line">   Transfer Size (Bytes)Bandwidth(MB&#x2F;s)</span><br><span class="line">   3355443210817.1</span><br><span class="line"></span><br><span class="line"> Device to Host Bandwidth, 1 Device(s)</span><br><span class="line"> PINNED Memory Transfers</span><br><span class="line">   Transfer Size (Bytes)Bandwidth(MB&#x2F;s)</span><br><span class="line">   3355443212143.6</span><br><span class="line"></span><br><span class="line"> Device to Device Bandwidth, 1 Device(s)</span><br><span class="line"> PINNED Memory Transfers</span><br><span class="line">   Transfer Size (Bytes)Bandwidth(MB&#x2F;s)</span><br><span class="line">   33554432742271.1</span><br><span class="line"></span><br><span class="line">Result &#x3D; PASS</span><br><span class="line"></span><br><span class="line">NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.</span><br></pre></td></tr></table></figure><h2 id="install-cudnn">Install cuDNN</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">wget https://developer.nvidia.com/compute/machine-learning/cudnn/secure/v7.1.1/prod/9.1_20180214/Ubuntu16_04-x64/libcudnn7_7.1.1.5-1+cuda9.1_amd64</span><br><span class="line">wget https://developer.nvidia.com/compute/machine-learning/cudnn/secure/v7.1.1/prod/9.1_20180214/Ubuntu16_04-x64/libcudnn7-dev_7.1.1.5-1+cuda9.1_amd64</span><br><span class="line">wget https://developer.nvidia.com/compute/machine-learning/cudnn/secure/v7.1.1/prod/9.1_20180214/Ubuntu16_04-x64/libcudnn7-doc_7.1.1.5-1+cuda9.1_amd64</span><br><span class="line"></span><br><span class="line">sudo dpkg -i libcudnn7_7.1.1.5-1+cuda9.1_amd64.deb</span><br><span class="line">sudo dpkg -i libcudnn7-dev_7.1.1.5-1+cuda9.1_amd64.deb</span><br><span class="line">sudo dpkg -i libcudnn7-doc_7.1.1.5-1+cuda9.1_amd64.deb</span><br></pre></td></tr></table></figure><h2 id="compile-samples-and-test-them-1">Compile samples and test them</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cp -r /usr/src/cudnn_samples_v7/ <span class="variable">$HOME</span>/cuda</span><br><span class="line"><span class="built_in">cd</span> cudnn_samples_v7/mnistCUDNN/</span><br><span class="line">make clean &amp;&amp; make</span><br></pre></td></tr></table></figure><h2 id="check-installation-by-running-mnistcudnn">Check installation by running <em>mnistCUDNN</em></h2><p>You should see an output similar to these ones:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">./mnistCUDNN</span><br><span class="line">cudnnGetVersion() : 7101 , CUDNN_VERSION from cudnn.h : 7101 (7.1.1)</span><br><span class="line">Host compiler version : GCC 5.4.0</span><br><span class="line">There are 1 CUDA capable devices on your machine :</span><br><span class="line">device 0 : sms 80  Capabilities 7.0, SmClock 1530.0 Mhz, MemSize (Mb) 16160, MemClock 877.0 Mhz, Ecc=1, boardGroupID=0</span><br><span class="line">Using device 0</span><br><span class="line"></span><br><span class="line">Testing single precision</span><br><span class="line">Loading image data/one_28x28.pgm</span><br><span class="line">Performing forward propagation ...</span><br><span class="line">Testing cudnnGetConvolutionForwardAlgorithm ...</span><br><span class="line">Fastest algorithm is Algo 5</span><br><span class="line">Testing cudnnFindConvolutionForwardAlgorithm ...</span><br><span class="line">^^^^ CUDNN_STATUS_SUCCESS <span class="keyword">for</span> Algo 0: 0.026624 time requiring 0 memory</span><br><span class="line">^^^^ CUDNN_STATUS_SUCCESS <span class="keyword">for</span> Algo 1: 0.046080 time requiring 3464 memory</span><br><span class="line">^^^^ CUDNN_STATUS_SUCCESS <span class="keyword">for</span> Algo 2: 0.046080 time requiring 57600 memory</span><br><span class="line">^^^^ CUDNN_STATUS_SUCCESS <span class="keyword">for</span> Algo 7: 0.062464 time requiring 2057744 memory</span><br><span class="line">^^^^ CUDNN_STATUS_SUCCESS <span class="keyword">for</span> Algo 5: 0.078848 time requiring 203008 memory</span><br><span class="line">Resulting weights from Softmax:</span><br><span class="line">0.0000000 0.9999399 0.0000000 0.0000000 0.0000561 0.0000000 0.0000012 0.0000017 0.0000010 0.0000000 </span><br><span class="line">Loading image data/three_28x28.pgm</span><br><span class="line">Performing forward propagation ...</span><br><span class="line">Resulting weights from Softmax:</span><br><span class="line">0.0000000 0.0000000 0.0000000 0.9999288 0.0000000 0.0000711 0.0000000 0.0000000 0.0000000 0.0000000 </span><br><span class="line">Loading image data/five_28x28.pgm</span><br><span class="line">Performing forward propagation ...</span><br><span class="line">Resulting weights from Softmax:</span><br><span class="line">0.0000000 0.0000008 0.0000000 0.0000002 0.0000000 0.9999820 0.0000154 0.0000000 0.0000012 0.0000006 </span><br><span class="line"></span><br><span class="line">Result of classification: 1 3 5</span><br><span class="line"></span><br><span class="line">Test passed!</span><br><span class="line"></span><br><span class="line">Testing half precision (math <span class="keyword">in</span> single precision)</span><br><span class="line">Loading image data/one_28x28.pgm</span><br><span class="line">Performing forward propagation ...</span><br><span class="line">Testing cudnnGetConvolutionForwardAlgorithm ...</span><br><span class="line">Fastest algorithm is Algo 5</span><br><span class="line">Testing cudnnFindConvolutionForwardAlgorithm ...</span><br><span class="line">^^^^ CUDNN_STATUS_SUCCESS <span class="keyword">for</span> Algo 0: 0.024576 time requiring 0 memory</span><br><span class="line">^^^^ CUDNN_STATUS_SUCCESS <span class="keyword">for</span> Algo 2: 0.044032 time requiring 28800 memory</span><br><span class="line">^^^^ CUDNN_STATUS_SUCCESS <span class="keyword">for</span> Algo 1: 0.048128 time requiring 3464 memory</span><br><span class="line">^^^^ CUDNN_STATUS_SUCCESS <span class="keyword">for</span> Algo 7: 0.058368 time requiring 2057744 memory</span><br><span class="line">^^^^ CUDNN_STATUS_SUCCESS <span class="keyword">for</span> Algo 5: 0.077824 time requiring 203008 memory</span><br><span class="line">Resulting weights from Softmax:</span><br><span class="line">0.0000001 1.0000000 0.0000001 0.0000000 0.0000558 0.0000001 0.0000011 0.0000017 0.0000010 0.0000001 </span><br><span class="line">Loading image data/three_28x28.pgm</span><br><span class="line">Performing forward propagation ...</span><br><span class="line">Resulting weights from Softmax:</span><br><span class="line">0.0000000 0.0000000 0.0000000 1.0000000 0.0000000 0.0000703 0.0000000 0.0000000 0.0000000 0.0000000 </span><br><span class="line">Loading image data/five_28x28.pgm</span><br><span class="line">Performing forward propagation ...</span><br><span class="line">Resulting weights from Softmax:</span><br><span class="line">0.0000000 0.0000008 0.0000000 0.0000002 0.0000000 1.0000000 0.0000154 0.0000000 0.0000012 0.0000006 </span><br><span class="line"></span><br><span class="line">Result of classification: 1 3 5</span><br><span class="line"></span><br><span class="line">Test passed!</span><br></pre></td></tr></table></figure><h2 id="install-final-command-line-utilities-and-setup-ld_library_path">Install final command line utilities and setup LD_LIBRARY_PATH</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install cuda-command-line-tools-9-1</span><br><span class="line"><span class="built_in">export</span> LD_LIBRARY_PATH=<span class="variable">$LD_LIBRARY_PATH</span>:/usr/<span class="built_in">local</span>/cuda/extras/CUPTI/lib64</span><br></pre></td></tr></table></figure><h2 id="check-tensorflow-is-running-on-a-gpu">Check tensorflow is running on a GPU</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">pip install --upgrade tensorflow-gpu</span><br><span class="line">python</span><br><span class="line"></span><br><span class="line">import tensorflow as tf</span><br><span class="line">sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))</span><br><span class="line"></span><br><span class="line">2018-03-20 16:43:50.112739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node <span class="built_in">read</span> from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span><br><span class="line">2018-03-20 16:43:50.112955: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212] Found device 0 with properties: </span><br><span class="line">name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53</span><br><span class="line">pciBusID: 0000:00:1e.0</span><br><span class="line">totalMemory: 15.78GiB freeMemory: 307.94MiB</span><br><span class="line">2018-03-20 16:43:50.112982: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0</span><br><span class="line">2018-03-20 16:43:50.802739: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10 MB memory) -&gt; physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1e.0, compute capability: 7.0)</span><br><span class="line">Device mapping:</span><br><span class="line">/job:localhost/replica:0/task:0/device:GPU:0 -&gt; device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1e.0, compute capability: 7.0</span><br><span class="line">2018-03-20 16:43:50.805298: I tensorflow/core/common_runtime/direct_session.cc:297] Device mapping:</span><br><span class="line">/job:localhost/replica:0/task:0/device:GPU:0 -&gt; device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1e.0, compute capability: 7.0</span><br></pre></td></tr></table></figure><p>From the output logs, we can see that Tensorflow detected our GPU device. We can now train our models on the GPU and accelerate the training time.</p>]]></content>
      
      
      
        <tags>
            
            <tag> performance </tag>
            
            <tag> machine-learning </tag>
            
            <tag> gpu </tag>
            
            <tag> cuda </tag>
            
            <tag> nvidia </tag>
            
            <tag> tensorflow </tag>
            
            <tag> deep-learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Building a fast inference service with falcon and bjoern</title>
      <link href="/2018/01/06/fast-inference-falcon-bjoern/"/>
      <url>/2018/01/06/fast-inference-falcon-bjoern/</url>
      
        <content type="html"><![CDATA[<p>A good portion of time in building supervised machine learning models is spent into training, that is, finding the best set of parameters that will give us the best accuracies on unseen data. Once we are satisfied with the obtained results, we often need to deploy and make it available to answer queries from a wide range of sources.</p><p>We can elaborate complex scenarios that are able to scale and answer to thousands of requests. However, let us consider that we need to prototype and showcase a quick solution, without sacrificing performance and scalability.</p><p>For this scenario we can combine the <a href="https://falconframework.org">Falcon</a> framework, which is a highly optimized and reliable web framework with <a href="https://github.com/jonashaag/bjoern">bjoern</a>, a very lightweight and fast WSGI server. This post shows a possible use of these tools. For the inference model, we will use a pre-trained model built with the <a href="https://fasttext.cc">fasttext</a> classification tool. This model is able to classify text according to its polarity.</p><h2 id="installing-dependencies">Installing dependencies</h2><ul><li>Optional virtual environment</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">virtualenv -p python3 venv</span><br></pre></td></tr></table></figure><ul><li>Falcon and Bjoern</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pip install ujson</span><br><span class="line">pip install cython</span><br><span class="line">pip install --no-binary :all: falcon </span><br><span class="line">pip install bjoern</span><br></pre></td></tr></table></figure><ul><li>FastText</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;facebookresearch&#x2F;fastText.git</span><br><span class="line">cd fastText</span><br><span class="line">pip install pybind11</span><br><span class="line">python setup.py install</span><br><span class="line">cd ..</span><br></pre></td></tr></table></figure><h2 id="download-the-pre-trained-model">Download the pre-trained model</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir models</span><br><span class="line">wget --directory-prefix&#x3D;models https:&#x2F;&#x2F;s3-us-west-1.amazonaws.com&#x2F;fasttext-vectors&#x2F;supervised_models&#x2F;amazon_review_full.ftz</span><br></pre></td></tr></table></figure><h2 id="creating-a-rest-resource">Creating a REST resource</h2><p>In order to serve requests by answering with polarity predictions (number of stars), let's define a resource by specifying a REST endpoint.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> fastText</span><br><span class="line"><span class="keyword">import</span> falcon</span><br><span class="line"><span class="keyword">import</span> bjoern</span><br><span class="line"><span class="keyword">import</span> ujson</span><br><span class="line"></span><br><span class="line">REVIEW_MODEL = <span class="string">&#x27;models/amazon_review_full.ftz&#x27;</span></span><br><span class="line">WEB_HOST = <span class="string">&#x27;127.0.0.1&#x27;</span></span><br><span class="line">PORT = <span class="number">9000</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">&#x27;Loading amazon review polarity model ...&#x27;</span>)</span><br><span class="line">review_classifier = fastText.load_model(REVIEW_MODEL)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ReviewResource</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">on_post</span>(<span class="params">self, req, resp</span>):</span></span><br><span class="line">form = req.params</span><br><span class="line"><span class="keyword">if</span> <span class="string">&#x27;text&#x27;</span> <span class="keyword">in</span> form <span class="keyword">and</span> form[<span class="string">&#x27;text&#x27;</span>]:</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">classification, confidence = review_classifier.predict(form[<span class="string">&#x27;text&#x27;</span>])</span><br><span class="line">resp.body = ujson.dumps(&#123;<span class="string">&#x27;&#123;&#125; star&#x27;</span>.<span class="built_in">format</span>(classification[<span class="number">0</span>][-<span class="number">1</span>]) : confidence[<span class="number">0</span>]&#125;)</span><br><span class="line">resp.status = falcon.HTTP_200</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">resp.body = ujson.dumps(&#123;<span class="string">&#x27;Error&#x27;</span>: <span class="string">&#x27;An internal server error has occurred&#x27;</span>&#125;)</span><br><span class="line">resp.status = falcon.HTTP_500</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    resp.body = ujson.dumps(&#123;<span class="string">&#x27;Error&#x27;</span>: <span class="string">&#x27;param \&#x27;text\&#x27; is mandatory&#x27;</span>&#125;)</span><br><span class="line">    resp.status = falcon.HTTP_400</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># instantiate a callable WSGI app</span></span><br><span class="line">app = falcon.API()</span><br><span class="line"></span><br><span class="line"><span class="comment"># long-lived resource class instance</span></span><br><span class="line">infer_review = ReviewResource()</span><br><span class="line"></span><br><span class="line"><span class="comment"># handle all requests to the &#x27;/inferreview&#x27; URL path</span></span><br><span class="line">app.req_options.auto_parse_form_urlencoded = <span class="literal">True</span></span><br><span class="line">app.add_route(<span class="string">&#x27;/inferreview&#x27;</span>, infer_review)</span><br><span class="line"></span><br><span class="line">print(<span class="string">&#x27;Listening on&#x27;</span>, WEB_HOST + <span class="string">&#x27;:&#x27;</span> + <span class="built_in">str</span>(PORT) + <span class="string">&#x27;/inferreview&#x27;</span>)</span><br><span class="line">bjoern.run(app, WEB_HOST, PORT, reuse_port=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>In the above code, we define an handler for POST requests, instantiate the application, configure routing and finally run the bjoern WSGI server.</p><h2 id="making-queries">Making queries</h2><p>The command bellow allow us to make queries to our web server. As an example, the request asks for a rating of the following review: 'I love this product.'</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -X POST http:&#x2F;&#x2F;localhost:9000&#x2F;inferreview -H &#39;Content-Type: application&#x2F;x-www-form-urlencoded&#39; -d text&#x3D;&quot;I love this product.&quot;</span><br></pre></td></tr></table></figure><p>which gives the desired classification, along with its confidence:</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">&quot;5 star&quot;</span>: <span class="number">0.7544972301</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="how-does-it-scale">How does it scale ?</h2><p>We have a server answering to client queries. We can make a quick test in order assess the scalability of our system. The <a href="https://github.com/giltene/wrk2">wrk2</a> tool is perfect for this since it allows to record the latency distribution for different throughput (request per second) values.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;giltene&#x2F;wrk2.git</span><br><span class="line">cd wrk2</span><br><span class="line">make</span><br><span class="line">.&#x2F;wrk -t4 -c400 -d30s -R25000 -L -s scripts&#x2F;post.lua http:&#x2F;&#x2F;127.0.0.1:9000&#x2F;inferreview</span><br></pre></td></tr></table></figure><p>The above command tests our server using 4 client threads, keeping 400 connections open, during 30 seconds and with a constant throughput of 20000 per second. The file scripts/post.lua is also modified as follows:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">wrk.method &#x3D; &quot;POST&quot;</span><br><span class="line">wrk.body   &#x3D; &quot;text&#x3D;&#39;awesome product&#39;&quot;</span><br><span class="line">wrk.headers[&quot;Content-Type&quot;] &#x3D; &quot;application&#x2F;x-www-form-urlencoded&quot;</span><br><span class="line">wrk.headers[&quot;Cache-Control&quot;] &#x3D; &quot;no-cache&quot;</span><br></pre></td></tr></table></figure><p>The wrk2 tool generates a large output, containing a complete report with statistics in the <a href="https://github.com/HdrHistogram/HdrHistogram">HdrHistogram</a> (High Dynamic Range Histogram) format, which we can use to make a plot of different throughput rates, as shown in the figure bellow:</p><p><img src="/images/falcon-bjoern/falcon-bjoern-latency.png" /></p><p>We can see the server handles 99% of all requests under 60 milliseconds, even when the throughput is 20000 requests per second, which is a fairly good performance. As a further comparison, the plot bellow shows a comparison with the popular wsgi server <a href="http://gunicorn.org">gunicorn</a>, with the same constant throughput rate.</p><p><img src="/images/falcon-bjoern/bjoern-gunicorn.png" /></p><p>The gunicorn server ran with 9 asynchronous worker processes, based on <a href="http://www.gevent.org">gevent</a> threads and it was called with the following command:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gunicorn polarity_server:app -w 9 -k gevent</span><br></pre></td></tr></table></figure><p>It is clear that the bjoern server handled requests faster than gunicorn, and thus is a strong and faster alternative. These tests were performed on a standard laptop with 4 cores.</p><h2 id="final-remarks">Final remarks</h2><p>Falcon and Bjoern make a great combination to quickly serve thousands of requests with low effort and also make a good starting point for more complex scenarios. This speedup is justified since Falcon itself was compiled with <a href="http://cython.org">Cython</a>. We could have used the <a href="http://pypy.org">pypy</a> python implementation alternative for even faster results. On the other hand, Bjoern is a very lightweight wsgi server, with a very low memory footprint, and single-threaded, which avoids locking overheads such as the <a href="https://wiki.python.org/moin/GlobalInterpreterLock">GIL</a>. The right combination of tools, allow us to worry less on performance issues and focus more on implementing the task that we want to provide. Finally, the source code is available <a href="https://github.com/AlexPnt/falcon-polarity-inference">here</a>.</p>]]></content>
      
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> performance </tag>
            
            <tag> machine-learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Building a machine-learning pipeline with scikit-learn and Qt - Part VI</title>
      <link href="/2017/09/10/ml-pipeline-6/"/>
      <url>/2017/09/10/ml-pipeline-6/</url>
      
        <content type="html"><![CDATA[<p>In the learning phase a set of examples are shown to the classifier, with class labels (supervised learning) or without them (unsupervised learning). As a result, the classifier is then able to categorize, with a certain accuracy, new unseen data. Experimental evaluation is also an important step to assess the effectiveness of a classifier, i.e, the quality of the decisions made by a predictive model on unseen data. This post is a follow up from the <a href="/2017/09/09/ml-pipeline-5/">previous post</a> and describes some classical and well-known learning methods available and how to evaluate their performance.</p><h2 id="classifiers">Classifiers</h2><p>Some common learning algorithms are described bellow:</p><ul><li><p><strong>Minimum Distance Classifier</strong> - Minimum Distance Classifiers are template matching system where unseen feature vectors are matched against template prototypes (vectors) representing the classes. This matching usually involves computing the matching error between both feature vectors using for example the euclidean distance as the error metric. This matching distance can simply be stated as: <span class="math display">\[ || x - m_k|| \]</span> where <em>x</em> is the unseen feature vector and <em>m<sub>k</sub></em> is the feature prototype for class <em>k</em>, usually consisting in a feature vector with feature mean values obtained during the training phase. After <em>k</em> tests are performed, the class belonging to the prototype with the minimum distance is chosen.</p></li><li><p><strong>k-Nearest Neighbor (kNN)</strong> - The kNN is a simple and fast unsupervised algorithm that classifies unknown instances based on the majority class from <em>k</em> neighbors. This method starts by constructing a distance matrix between the training samples and the new test sample and chooses the nearest <em>k</em> neighbors. Afterwards, the majority label from these neighbors is assigned to the new data. For two-class problems, <em>k</em> is usually an odd number to prevent ties.</p></li><li><p><strong>Naive Bayes</strong> - The Naive Bayes Classifier is a simple supervised probabilistic model based on the Bayes’ theorem. The posterior probabilities are computed for each class w<sub>j</sub> and the class with largest outcome is chosen to classify the feature vector <em>x</em>. To achieve this result, the likelihood, prior and evidence probabilities must be calculated, as shown below: <span class="math display">\[ posterior \, probability =  { likelihood \times prior \; probability \over evidence} \]</span> <span class="math display">\[ P(w_j | x) =  { p(x|w_j) \times P(w_j)  \over  p(x) } \]</span> The main difficulty is to compute the likelihoods <em>p(x|ω<sub>j</sub>)</em> since the other factors are obtained from the data. <em>P(ω<sub>j</sub>)</em> are the prior probabilities and <em>p(x)</em> is a normalization factor which is sometimes omitted. Assuming independence of the features, the likelihoods can be obtained as shown bellow: <span class="math display">\[ P(x | w_j) =  { \prod P(x_k | w_j) } \]</span> This simplification allows to compute the class conditional probabilities for each feature separately, reducing complexity and computational costs.</p></li><li><p><strong>Support Vector Machines (SVM)</strong> - Support vector machines are optimization methods for binary classification tasks that map the features to a higher dimensional space where an optimal linear hyperplane that separates the existing classes exists. The decision boundary is chosen with the help of some training examples, called the support vectors, that have the widest separation between them and help maximizing the margin between the boundaries of the different classes. The decision surface is in the “middle” of these vectors. During the training phase, the coefficient vector <em>w</em> and the constant <em>b</em> that define the separating hyperplane are searched such that the following error function is minimized: <span class="math display">\[ minimize: {1 \over 2} ||w||^2 + C \sum ξ_i \]</span> <span class="math display">\[  s.t. \quad y_i(φ(x_i) + b) \geq 1 - ξ_i \]</span> where <em>C</em> is an arbitrary constant and <em>ξ</em> are slack variables used to penalize misclassified instances that increase with the distance from the margin. If <em>C</em> is large, the penalty for misclassification is greater. The vectors <em>x<sub>i</sub></em> are the training instances. This method makes use of a kernel function <em>φ</em> used to transform the input vectors into higher dimensional vectors. New instances are then classified according to which ”side” of the hyperplane they are located.</p></li><li><p><strong>Decision Tree</strong> - Decision Trees are supervised methods that learn rules based on the training data to classify new instances. The built trees are simple to understand and visualize. During training, each node of the tree is recursively split by the feature that provides, for example, the best information gain.</p></li><li><p><strong>Random Forest</strong> - Random forests are ensemble methods, i.e., they take into consideration the prediction of several classifiers in order to improve the accuracy and robustness of the prediction results. In the case of random forests, they train a set of random trees with bootstrapped samples (samples drawn with replacement) from the original training data. Each tree is grown by selecting m random features from the d features and recursively splitting the data by the best splits. The classification of new data is achieved by a majority vote.</p></li></ul><p>Each classifier has its own advantages and disadvantages and the right technique should be chosen according to the requirements of the problem-at-hand.</p><h2 id="evaluation-metrics">Evaluation Metrics</h2><p>In classification tasks, predictions made by a classifier are either considered Positive or False (under some category) and the expected judgments are called True or False (again, under a certain category). Common metrics are:</p><ul><li><p><strong>Accuracy</strong> - This measure provides a proportion of correctly classified instances and correctly rejected instances (True Positives and True Negatives) among the whole dataset. <span class="math display">\[ Acc = {TP + TN \over TP + TN + FP + FN}\]</span></p></li><li><p><strong>Precision</strong> - This measure provides a proportion of correctly classified instances (True Positives) among all the positive identified instances (True Positives and False Positives). <span class="math display">\[ P = {TP \over TP + FP }\]</span></p></li><li><p><strong>Recall</strong> - This measure, sometimes called sensitivity, provides a proportion of correctly classified instances (True Positives) among the positive instances that were and should have been correctly identified, i.e., the whole positive part of the dataset (True Positives and False Negatives). <span class="math display">\[ R = {TP \over TP + FN }\]</span></p></li><li><p><strong>F-measure</strong> - This measure combines precision and recall and provides a balance between them. It is computed as the harmonic mean between the two metrics providing the same weight for both. <span class="math display">\[ F_1 = {2 \times P \times R \over P +  R }\]</span></p></li><li><p><strong>Stratified K-fold Cross Validation</strong> - A popular technique to evaluate the performance of the system is to split the data into training and testing sets, using the later to estimate the true generalization performance of the classifier. However, this may bring some issues such as the trade-offs between the percentage splits or the representativity of the test set. A popular accepted approach is to split the entire dataset into k representative partitions, using k − 1 of these partitions for training and the remaining one for testing. This process is then repeated <em>k</em> times, (each time using a different test partition) and the results averaged.</p></li></ul><p><img src="/images/ml-pipeline/metrics.png" /></p><h2 id="visualizing-the-performance">Visualizing the performance</h2><ul><li><strong>Receiver Operating Characteristics (ROC)</strong> - ROC curves are useful graphs to visualize the performance of binary classifiers. They are useful to compare the rates at which the classifier is making correct predictions (True Positive Rate plotted on the Y axis) against the rate of false alarms (False Positive Rate plotted on the X axis). Important points in this graph are the lower left point (0,0), representing a classifier that never classify positive instances, neither having False Positives or True Positives. On the other hand, the upper right point (1,1) represents a classifier that classifies every instance as positive, disregarding if it is a false positive or not. Finally, the point (0,1) represents the perfect classifier, where every instance was correctly classified. The area bellow the ROC curve is called Area Under the Curve (AUC) and is also a good evaluation measure. A perfect classifier would have an AUC of 1.0 while a random classifier would only have 0.5.</li></ul><p><img src="/images/ml-pipeline/roc.png" /></p><ul><li><strong>Precision-Recall Curves (PR)</strong> - The Precision-Recall Curve plots the trade-off between the precision and recall achieved by a classifier, by showing the recall on the X axis and precision on the Y axis. An important point in this graph is the upper right point (1,1) which represents the ideal classifier having maximum precision and recall. Figure 2.4 shows three hypothetical classifiers and the areas of good and bad performance, which are above or below the line defined by a random classifier. The area bellow the PR curve is called Average Precision (AP) and is also a good measure. A perfect classifier would have an Average Precision of 1.0 while a random classifier would only have 0.5.</li></ul><p><img src="/images/ml-pipeline/precision-recall.png" /></p><p>In this series of posts, a machine learning application was presented, showing a typical pipeline with the following steps:</p><ul><li><p>Dataset Loading</p></li><li><p>Feature Assessment where the distribution of features are inspected and visualized</p></li><li><p>Preprocessing where data transformations are applied and issues such as dataset balancing are addressed.</p></li><li><p>Feature Selection methods where features with he most discriminative power are selected</p></li><li><p>Classifications performed by predictive models where their performance is evaluated using ROC curves, Precision-Recall Curves and generic metrics such as Accuracy, Precision, Recall and F1 measures.</p></li></ul><p>The machine learning method is an iterative process, where one needs to go back and forth, setting different parameters, and different methods in order to see which combination works best and provides the best results. You can checkout this application and try it for yourself. The source code is available in <a href="https://github.com/AlexPnt/Default-Credit-Card-Prediction">this repository</a>.</p>]]></content>
      
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> machine-learning </tag>
            
            <tag> scikit-learn </tag>
            
            <tag> qt5 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Building a machine-learning pipeline with scikit-learn and Qt - Part V</title>
      <link href="/2017/09/09/ml-pipeline-5/"/>
      <url>/2017/09/09/ml-pipeline-5/</url>
      
        <content type="html"><![CDATA[<p>In <a href="/2017/09/09/ml-pipeline-4/">part IV</a> some data preprocessing techniques were shown. After we have clean data, it is important to choose a set of features to use. This post presents an overview of some of the available methods that we can use.</p><p><img src="/images/ml-pipeline/feature_selection.png" /></p><h2 id="feature-selection">Feature Selection</h2><p>Feature selection usually involves selecting a subset of the original set of features that provide the biggest discriminatory power, i.e., are able to provide the best separation between classes, result in the best performance of the classifier when trained and avoid the curse of dimensionality. In fact, it has been said that exists a critical feature dimension from where the performance degrades rapidly. Feature selection helps in removing irrelevant and redundant features and is usually divided into filter methods, where a subset of features is selected, without considering the predictive model and wrapper methods which use the classifier to rank and choose the best feature set. This step, in conjunction with feature reduction, is likely to be one of the most important steps in the pipeline. The following list shows common techniques employed in feature selection:</p><ul><li><p><strong>Information Gain</strong> - Information gain ranks each attribute by its ability to discriminate the pattern classes. Features very informative will provoke the greatest decrease in entropy when the dataset is split by that feature. <span class="math display">\[ IG(S,A) =  H(S) - { H(S) \over \sum { |S_v| \over |S|} H(S_v)} \]</span> The Information Gain <em>IG</em> of a feature <em>A</em> is then equal to the decrease in entropy achieved by splitting the dataset <em>S</em> by feature values <em>v</em> into smaller sets <em>Sv</em> , and subtracting from the original entropy <em>H(S)</em> the average entropy of the splits.</p></li><li><p><strong>Gain Ratio</strong> - The gain ratio is simply the information gain normalized with the entropy of the feature <em>H(A)</em>. Since features with many values create lots of branches, creating a bias towards these features, the gain ratio corrects this by taking into account the number and size of branches of a split. <span class="math display">\[ GR(S,A) = {IG(S,A) \over H(A) }\]</span></p></li><li><p><strong>Fisher Score</strong> - The Fisher’s score select features with high class-separability and low class-variability and is defined by: <span class="math display">\[ F(x) = {|m_1 - m_2|^2 \over s_1^2 + s_2^2 }\]</span> where <em>m<sub>1</sub></em> and <em>m<sub>2</sub></em> are the means from the feature values (for a 2-class problem) and <em>s<sub>1</sub></em> and <em>s<sub>2</sub></em> are the standard deviations from the feature values of each class.</p></li><li><p><strong>Pearson Correlation</strong> - Pearson correlation can be used to select features that are highly correlated with the target class and features with low correlation between them. It is a useful measure to see how strongly related two features are. The Pearson correlation gives values between -1 and 1, where absolute values closer to 1 mean a high correlation. The sign indicates whether there is a positive relationship between the variables, that is, if a feature value increase or decreases, the other increases/decreases as well (positive correlation) or if one variable increases/decreases, the other decreases/increases with it (negative correlation). Naturally, we are interested in high absolute values for feature-class relationships and low values for feature-feature relationships. This Pearson correlation ρ is defined as: <span class="math display">\[ ρ(a,b) = {covariance(a,b) \over  σ(a) \times σ(b) }\]</span> where a and b can be both feature vectors or a feature vector and a label vector.</p></li><li><p><strong>Chi-Square Test</strong> - The Chi-Square test ranks each attribute by computing the χ2 statistic relative to the target class. This statistic is useful to see if there is an independence relationship between the features and the target class. High values of this statistic means that there is a strong dependence between a particular feature and the target class leading to its selection for classification tasks.</p></li></ul><h2 id="feature-reduction">Feature Reduction</h2><p>Feature reduction is also an important step that helps further reducing the dimensionality of the problem, reduces the complexity of the problem and decreases the computational costs. In order to reduce the dimensionality, it is important to keep only the most relevant, informative and discriminative features. Techniques such as Principal Component Analysis (PCA) may be employed in this stage, by aggregating multiple features through linear combinations.</p><ul><li><strong>Principal Component Analysis (PCA)</strong> - PCA is a popular unsupervised (ignores class labels) dimensionality reduction technique that uses linear transformations to find the directions (principal components) with the highest variance. It projects the original data into a lower dimensional space, with the new features retaining most of information. PCA works by finding vectors (called eigenvectors) with the highest amplitude (called eigenvalues) that represent the axes with the highest variance. The original data is then project onto these axes.</li></ul><p>Successful removal of noisy and redundant data typically improves the overall classification accuracy of the resulting model, reducing also overfitting. After every preprocessing and feature engineering step is completed, comes the learning and evaluation phase which will be discussed in the <a href="/2017/09/10/ml-pipeline-6/">next post</a>.</p>]]></content>
      
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> machine-learning </tag>
            
            <tag> scikit-learn </tag>
            
            <tag> qt5 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Building a machine-learning pipeline with scikit-learn and Qt - Part IV</title>
      <link href="/2017/09/09/ml-pipeline-4/"/>
      <url>/2017/09/09/ml-pipeline-4/</url>
      
        <content type="html"><![CDATA[<p><a href="/2017/09/03/ml-pipeline-3/">Part III</a> explored data visualization and distribution methods. This article shows a list of common preprocessing methods that may be applied.</p><p><img src="/images/ml-pipeline/preprocessing.png" /></p><h2 id="dataset-transformation">Dataset Transformation</h2><p>Some learning algorithms do not deal with certain types of data. In order to be able to use them, the type of the attributes might be transformed to another suitable type. For example converting symbolic features to equivalent numeric features, or the other way around. Another important issue to consider is the normalization of the data, such as converting different attributes to the same scale, avoiding the dominance of some attributes and decreasing the dispersion of the data.</p><ul><li><p><strong>Standardization</strong> - Standardization transforms each feature by removing their mean and diving non-constant features by their standard deviation, obtaining data normally distributed with zero mean and unit variance (<em>ρ<sub>Xstd</sub> = 0</em>, <em>σ<sub>Xstd</sub> = 1</em>). <span class="math display">\[ x&#39; = { x - ρX \over σX } \]</span></p></li><li><p><strong>Normalization</strong> - Normalization involves scaling samples/features vectors to have unit norm, usually achieved by diving by the euclidean norm of the vector. <span class="math display">\[ x&#39; = { x  \over \sqrt \sum x_i^2 } \]</span></p></li><li><p><strong>Scaling</strong> - Scaling transform the features to lie between a minimum and a maximum value. Typical ranges are [0,1] or [-1,1]. <span class="math display">\[ x&#39;_{[0,1]} = { x - min_x \over max_x - min_x } \]</span></p></li></ul><h2 id="dataset-balancing">Dataset Balancing</h2><p>The original dataset contain an imbalance between the two classes, that is, there is 23364(∼78%) samples from class ’0’ and 6636(∼22%) from class ’1’, which can affect the classifier prediction ability. In order to address this issue, 2 groups of methods may be employed, i.e, methods based on the undersampling of the majority class and methods based on the oversampling of the minority class:</p><p><strong>Undersampling</strong></p><ul><li><p><strong>Random Majority Undersampling</strong> - Starts with a set consisting only of samples from the minority class and adds random samples from the majority classes (allowing duplicates).</p></li><li><p><strong>NearMiss-1</strong> - Selects samples from the majority class that are close to the minority class samples, by choosing the ones whose average to three closest minority samples are the smallest.</p></li><li><p><strong>NearMiss-3</strong> - Surround each minority sample with a number of majority samples and choose the ones which have the highest average distance to three closest minority samples.</p></li><li><p><strong>Neighbor Cleaning Rule</strong> - Remove noisy data (samples whose class differs from the majority class of at least two of its three nearest neighbor) and the three nearest neighbors that misclassify samples of the minority class.</p></li></ul><p><strong>Oversampling</strong></p><ul><li><p><strong>Random Majority Undersampling</strong> - Starts with a set consisting only of samples from the majority class and adds random samples from the minority classes (allowing duplicates).</p></li><li><p><strong>SMOTE</strong> - Oversamples the minority data by creating ”synthetic” samples from the minority class.</p></li></ul><p>Applying these transformations to the input data usually bring benefits to the final accuracy of the classifier. The <a href="/2017/09/09/ml-pipeline-5/">next post</a> will be dedicated to feature selection/reduction methods.</p>]]></content>
      
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> machine-learning </tag>
            
            <tag> scikit-learn </tag>
            
            <tag> qt5 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Building a machine-learning pipeline with scikit-learn and Qt - Part III</title>
      <link href="/2017/09/03/ml-pipeline-3/"/>
      <url>/2017/09/03/ml-pipeline-3/</url>
      
        <content type="html"><![CDATA[<p>The assessment and visualization of the distribution of the features is an useful task to better get a sense of the discriminative capability of the features and how they relate with each other. In <a href="/2017/09/02/ml-pipeline-2/">part II</a> we talked about the dataset and the data collecting process. In this post we take a look at some common tasks such as:</p><ul><li><strong>Normalized Histogram Distribution</strong> - A normalized histogram (bin counts divided by total sum) showing the proportion of each feature value is shown for a certain feature where bars are grouped by class.</li></ul><p><img src="/images/ml-pipeline/feature-inspection-hist.png" /></p><ul><li><strong>Box Plot</strong> - A box plot for visualizing the distribution of the feature values of each feature grouped by class.</li></ul><p><img src="/images/ml-pipeline/feature-inspection-boxplot.png" /></p><ul><li><strong>Pairwise Relationships</strong> - A graph plotting the distribution relationships between two features, resulting in a 2 × 2 matrix where each diagonal element showed either the histogram distribution or the kernel density estimation of each feature distribution. Non-diagonal elements included a scatter plot of the two features grouped by class.</li></ul><p><img src="/images/ml-pipeline/feature-inspection-pairwise.png" /></p><ul><li><strong>Empirical Cumulative vs Standard Normal Density Functions</strong> - The Empirical Cumulative Density Functions are also shown against the standard Normal curve, along with the Kolmogorov–Smirnov statistic useful to test if the two functions are drawn from the same distribution (Standard Normal, in this case).</li></ul><p><img src="/images/ml-pipeline/feature-inspection-edf.png" /></p><ul><li><strong>Pearson Correlations</strong> - In order to see how strongly related two features are, the Pearson Correlation matrix is also shown, giving values between -1 and 1, where absolute values closer to 1 mean a high correlation. The sign indicates whether there is a positive relationship between the variables, that is, if a feature value increase or decreases, the other increases/decreases as well (positive correlation) or if one variable increases/decreases, the other decreases/increases with it (negative correlation).</li></ul><p><img src="/images/ml-pipeline/feature-inspection-pearson.png" /></p><ul><li><strong>Two-Dimensional Principal Components Analysis</strong> - A visualization of the two main principal components (directions with the highest variance).</li></ul><p><img src="/images/ml-pipeline/feature-inspection-pca.png" /></p><ul><li><strong>Two-Dimensional Linear Discriminant Analysis</strong> - A visualization of the two first linear discriminant directions (directions with highest class-separability and lowest inner-class variance).</li></ul><p><img src="/images/ml-pipeline/feature-inspection-lda.png" /></p><p>The <a href="/2017/09/09/ml-pipeline-4/">next post</a> will be about preprocessing methods that we can apply to our data. Stay tuned.</p>]]></content>
      
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> machine-learning </tag>
            
            <tag> scikit-learn </tag>
            
            <tag> qt5 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Building a machine-learning pipeline with scikit-learn and Qt - Part II</title>
      <link href="/2017/09/02/ml-pipeline-2/"/>
      <url>/2017/09/02/ml-pipeline-2/</url>
      
        <content type="html"><![CDATA[<p>In order to run experiments, we need a <a href="https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients">data</a>. This dataset was the same used in a <a href="http://www.sciencedirect.com/science/article/pii/S0957417407006719">research study</a>, consisting of data payments gathered during October, 2005, from a bank in Taiwan, using the credit card holders from that bank as the study targets.</p><p>In the <a href="/2017/09/02/ml-pipeline-1/">first part</a> we talked about the process of machine learning. This post will discuss data gathering and the dataset.</p><h2 id="data-gathering">Data Gathering</h2><p>The first step is to collect the training and testing data such as sufficient and representative data. This usually implies tasks such as:</p><ul><li><p><strong>Data Integration</strong> - The initial data might come from different sources, with different formats or duplicated data. This step is important to create a single repository of data.</p></li><li><p><strong>Data Balancing</strong> - As is often the case with real data, the data is usually not uniformly distributed across the different classes. Consequently, the trained models tend to predict new data with the majority class. Techniques to artificially balance the data might be employed here, such as reducing (Undersampling) / increasing (Oversampling) the number of instances from the majority /minority classes. An other alternative might be performing a stratified sampling in order to keep a significant number of instances in each class.</p></li><li><p><strong>Cleaning</strong> - The quality of the data is important. Sometimes the data will have missing attributes or useless values. However, redundant or noisy data are important issues to be addressed, such as instances with very similar feature values, attributes easily deducted from others or outliers.</p></li></ul><h2 id="the-dataset">The dataset</h2><p>Each sample of the dataset contains 23 variables and a predictive binary label (Defaul payment: Yes = 1, No = 0). These 23 features are described as follows:</p><ul><li>X1: Amount of the given credit</li><li>X2: Gender (1 = male; 2 = female)</li><li>X3: Education (1 = graduate school; 2 = university; 3 = high school; 4 = others).</li><li>X4: Marital status (1 = married; 2 = single; 3 = others)</li><li>X5: Age (age in years)</li><li>X6-X11: History of past payment (X6 = the repayment status in September, 2005 ; . . .; X11 = the repayment status in April, 2005), where x = -1 = pay duly and x = 1,2,3, ..9 = payment delay for x months</li><li>X12-X17: Amount of bill statement</li><li>X18–X23: Amount of previous payment</li></ul><h2 id="the-gui">The GUI</h2><p>The resulting application has a tab designed to show a quick description of the loaded dataset, that is, the description of the features, the cardinality of each class and some statistical measures of each feature:</p><p><img src="/images/ml-pipeline/loading.png" /></p><p>For the design of the interface, the <a href="http://doc.qt.io/qt-5/qtdesigner-manual.html">Qt Designer</a> is a great tool to quickly generate our GUI. This tool generates a <em>.xml</em> file describing the UI, which is then used to generate a python module which we can import and use to load the main interface.</p><p>The extraction of statistical metrics is useful to get a quick overview of how our data is distributed. We will talk about these metrics later in detail.</p><p><a href="/2017/09/03/ml-pipeline-3/">Part III will provide an overview of feature assessment and visualization</a>.</p>]]></content>
      
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> machine-learning </tag>
            
            <tag> scikit-learn </tag>
            
            <tag> qt5 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Building a machine-learning pipeline with scikit-learn and Qt - Part I</title>
      <link href="/2017/09/02/ml-pipeline-1/"/>
      <url>/2017/09/02/ml-pipeline-1/</url>
      
        <content type="html"><![CDATA[<p>In machine learning problems, we usually describe the objects that we wish to recognize by a set of variables called features, consisting of information that we extract from the study objects. Such features are then collected in vectors of dimension d, called feature vectors, usually denoted by x. They represent points in a d dimensional space, called feature space. Each point belongs to a class, usually denoted by w and the combination of the feature vectors and their corresponding class are called patterns. If we only have two target classes, we are in the presence of a binary classification problem. Classification problems may be trivial for humans but are usually quite challenging for automated systems, since we need to deal with a lot of issues such as finding a reasonable number of distinguishing features good enough for classification, that are able to separate the target classes, or finding models that have good generalization capabilities (perform well on unseen data), avoiding overfitting of the model to the training data. The main goal is then to find the best decision boundary that results in the best generalization on testing data.</p><p>The figure below shows an hypothetical problem with two classes and two features. The main goal is then to find the best decision boundary that results in the best generalization on testing data.</p><p><img src="/images/ml-pipeline/decision_boundary.png" /></p><p>The next figure shows the workflow of a typical machine learning application. Usually, classification involves steps such as data gathering, preprocessing, feature selection/reduction, training, testing and the final evaluation. The main goal is to assign, with the best accuracy possible, new labels to new instances.</p><p><img src="/images/ml-pipeline/ml-pipeline.png" /></p><p>In this post, we will see how we can recognize credit card frauds using classic algorithms. Default credit cards are an important issue that bring negative consequences to both sides, i.e, banks and customer. If a customer does not pay his obligations, banks loose money, the customer will lose credibility in future payments, collection calls start to be made and in last resort, the case may go into the court. In order to avoid all of that trouble, effective methods that are able to predict the default of credit cards are needed. Therefore, default credit card prediction is an important, challenging and useful task that should be addressed.</p><p>Python is a great choice for the development of this kind of applications, along with a series of helpful packages containing statistical and learning methods, namely the <a href="http://www.numpy.org/">numpy</a> and <a href="https://www.scipy.org/">scipy</a> packages for numerical computations, <a href="http://matplotlib.org/">matplotlib</a> , <a href="http://pandas.pydata.org/">pandas</a> and <a href="http://seaborn.pydata.org/">seaborn</a> for data visualization, <a href="http://scikit-learn.org/stable/">sklearn</a>, <a href="https://github.com/rasbt/mlxtend">mlxtend</a> and <a href="https://github.com/scikit-learn-contrib/imbalanced-learn">unbalanced dataset</a> for the learning and feature selection / reduction methods and finally the <a href="https://www.riverbankcomputing.com/software/pyqt/intro">PyQt5</a> and <a href="http://doc.qt.io/qt-5/qtdesigner-manual.html">Qt Designer</a> for the Graphical User Interface (GUI).</p><p><a href="/2017/09/02/ml-pipeline-2/">Part II will look at the first step of the process: Data Gathering</a>.</p>]]></content>
      
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> machine-learning </tag>
            
            <tag> scikit-learn </tag>
            
            <tag> qt5 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Performance of Different NLP Toolkits in Formal and Social Text</title>
      <link href="/2017/08/27/nlp-performance/"/>
      <url>/2017/08/27/nlp-performance/</url>
      
        <content type="html"><![CDATA[<h2 id="motivation-for-this-work">Motivation for this work</h2><p>The Web is a large source of data, mostly expressed in natural language text. Natural language processing (NLP) systems need to understand the human languages in order to extract new knowledge and perform diverse tasks, such as information retrieval, machine translation, or text classification, among others. For widely-spoken languages, such as English, there is currently a wide range of NLP toolkits available for performing lower-level NLP tasks, including tokenization, part-of-speech (POS) tagging, chunking or named entity recognition (NER).</p><p>This enables that more complex applications do not have to be developed completely from scratch. Yet, with the availability of many such toolkits, the one to use is rarely obvious. Users have also to select the most suitable set of tools that meets their specific purpose. Among other aspects, the selection may consider the community of users, frequency of new versions and updates, support, portability, cost of integration, programming language, the number of covered tasks, and, of course, their performance. Most NLP tools available for English are made with good ease-of-use in mind, are open-source and are freely available, but it is not always clear which tool best fits the requirements of the user.</p><p>This post compares a range of natural language processing toolkits with their default configuration, while performing a set of standard tasks (e.g. tokenization, POS tagging, chunking and NER), in popular datasets that cover newspaper and social network text. This kind of comparison is helpful for other developers and researchers in need of making a similar selection.</p><h2 id="addressed-tasks">Addressed Tasks</h2><p>In order to evaluate how good standard NLP tools perform against different kinds of text, such as noisy text from social networks and formal text from newspapers, the performance of common NLP tasks was analysed. The addressed tasks were tokenization, POS-tagging, chunking and NER. The following list describes the four evaluated tasks:</p><ul><li><p><strong>Tokenization:</strong> usually the first step in NLP pipelines. It is the process of breaking down sentences into tokens, which can be words or punctuation marks. Although it seems a relatively easy task, it has some issues because some words may rise doubts on how they should be tokenized, namely words with apostrophes, or with mixed symbols.</p></li><li><p><strong>Part-of-Speech (POS) Tagging:</strong> given a specific tagset, it determines the part-of-speech of each token in a sentence. In this work, the tags of the <a href="https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html">Penn Treebank</a> Project, popular among the NLP community, were used.</p></li><li><p><strong>Chunking:</strong> also known as shallow parsing, it is a lighter syntactic parsing task. The main purpose is to identify the constituent groups in which the words are organized. This includes at least noun phrases (NP), verb phrases (VP) and prepositional phrases (PP). The sequence of chunks forms the entire sentence. They may also be nested inside each other to form a tree structure, where each leaf is a word, the previous node is the corresponding POS-tag and the head of the tree is the chunk type.</p></li><li><p><strong>Name Entity Recognition/Classification:</strong> deals with the identification of certain types of entities in a text and may go further classifying them into one of given categories, typically PERson, LOCations, ORGanizations, all proper nouns, and sometimes others, such as dates. Usually this task is also useful to link mentions in the text to a specific entity. This usually involves other sub-problems such as Name Entity Disambiguation (NED).</p></li></ul><h2 id="compared-tools">Compared Tools</h2><p>Tools can be implemented in different programming languages; have available models that cover different tasks, kinds of text or languages; require different setups; or have different learning curves for simple usage or for integration. The tools compared in this work were trained for English and are open, well-known and widely used by the NLP community. Moreover, they were developed either in Java or Python, which, nowadays, are probably the two languages more frequently used to develop NLP applications and for which there is a broader range of available toolkits. The compared tools are enumerated in the following list, where they are described and grouped in ``standard'' toolkits, which means they were developed with no specific kind of text in mind, and social network-oriented tools, which aim to be used in short messages from social networks.</p><h3 id="standard-nlp-toolkits">Standard NLP toolkits</h3><ul><li><p>The <a href="http://www.nltk.org"><strong>NLTK toolkit</strong></a> is a Python library aimed at individuals who are entering the NLP field. It is divided in independent modules, responsible for specific NLP tasks such as tokenization, stemming, tree representations, tagging, parsing and visualization. It also comes bundled with popular corpus samples ready to be read. By default, NLTK uses the Penn Treebank Tokenizer, which uses regular expressions to tokenize the text. Its PoS tagger uses the Penn Treebank tagset and is trained on the PENN Treebank corpus with a Maximum Entropy model. The Chunker and the NER modules are trained on the ACE corpus with a Maximum Entropy model.</p></li><li><p><a href="https://opennlp.apache.org"><strong>Apache OpenNLP</strong></a> is a Java library that uses machine learning methods for common natural language tasks, such as tokenization, POS tagging, NER, chunking and parsing. Users can either rely on pre-trained models for the previous tasks or train their own with a Perceptron or a Maximum Entropy. The pre-trained models for English PoS tagging and chunking use the Penn Treebank tagset. The Chunker is trained on the CoNLL-2000 dataset. The pre-trained NER models provided cover the recognition of persons, locations, organizations, time, date and percentage expressions.</p></li><li><p>The <a href="http://stanfordnlp.github.io/CoreNLP"><strong>Stanford CoreNLP</strong></a> toolkit is a Java pipeline that provides common language processing tasks. The most supported language is English, but other languages are also available. Comparing to other frameworks such as GATE or UIMA, CoreNLP is simple to set up and run, since users do not need to learn and understand complex installations and procedures. The CoreNLP performs a Penn Treebank style tokenization and the POS module is an implementation of the Maximum Entropy model using the Penn Treebank tagset. The NER component uses a Conditional Random Field (CRF) model and is trained on the CoNLL-2003 dataset.</p></li><li><p><a href="https://github.com/clips/pattern"><strong>Pattern</strong></a> is a Python library that provides modules for web mining, NLP and ML tasks. This library does not provide methods for a single field but rather a general cross-domain and ease-of-use functionality. The PoS tagger uses a simple rule-based model trained on the Brown Corpus.</p></li></ul><h3 id="social-network-oriented-toolkits">Social Network-Oriented Toolkits</h3><ul><li><p><a href="https://github.com/aritter/twitter_nlp"><strong>Alan Ritter's TwitterNLP</strong></a> is a Python library that offers a NLP pipeline for performing Tokenization, POS, Chunking and NER. The authors reduced the problem of dealing with noisy texts by developing a system based on a set of features extracted from Twitter-specific POS taggers, a dedicated shallow parsing logic, and the use of gazetteers generated from entities in the Freebase knowledge base, that best match the fleeting nature of informal texts.</p></li><li><p><a href="http://www.cs.cmu.edu/~ark/TweetNLP"><strong>CMU's TweetNLP</strong></a> is Java tool that provides a Tokenizer and a POS Tagger with available models, trained with a CRF model in Twitter data, manually annotated by its authors. In addition to the typical syntactic elements of a sentence, TweetNLP identifies content such as mentions, URLs, and emoticons.</p></li><li><p><a href="https://gate.ac.uk/wiki/twitie.html"><strong>TwitIE</strong></a> is an open-source plugin for GATE. The GATE framework comes already packaged with ANNIE, an information extraction system, and includes resources such as: a Tokenizer, a sentence splitter, gazetteer lists, a PoS tagger and a semantic tagger. TwitIE re-uses some of these components (sentence splitter and gazeteer lists) but adapts the other to the Twitter kind of text, supporting language identification, Tokenization, normalization, PoS tagging and Name Entity Recognition. The TwitIE tokenizer follows the same tokenization scheme as TwitterNLP. The PoS tagger uses an adptation of the Stanford tagger, trained on tweets with the Penn Tree Bank tagset, with additional tags for retweets, URLs, hashtags and user mentions.</p></li></ul><h2 id="datasets">Datasets</h2><p>In order to evaluate the performance of the different NLP toolkits and determine the best performing ones, the same criteria must be followed, including the same metrics and manually-annotated gold standard data. Testing tools in the same tasks and scenarios makes comparison fair and more reliable. This work used well-known datasets widely used in NLP and text classification research, not only in the evaluation of NLP tools, but also for training new models. More precisely, different gold standard datasets that cover different kinds of text -- newspaper and social media. Regarding newspaper text, a collection of news wire articles from the <a href="http://trec.nist.gov/data/reuters/reuters.html">Reuters Corpus</a> was used. The POS and chunking annotations of this dataset were obtained using a memory-based MBT tagger. The named entities were manually annotated at the University of Antwerp.</p><p>In order to represent social and more informal text, the annotated data from <a href="https://github.com/aritter/twitter_nlp/tree/master/data/annotated">Alan Ritter's Twitter corpus</a> was used, with manually tokenized, POS-tagged and chunked Twitter posts, also with annotated named entities. The collection of Twitter posts used in the <a href="http://oak.dcs.shef.ac.uk/msm2013/challenge.html">MSM 2013 workshop</a>, where named entities are annotated, was also used as a gold standard for social media text.</p><p>The POS tags of the CoNLL-2003 dataset follow the <a href="https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html">Penn Treebank style</a>. Alan Ritter's corpus follows the same format, with the same POS-tags and additional specific tags for retweets, <span class="citation" data-cites="usernames">@usernames</span>, #hashtags, and urls. For the chunk tags, the format I-TYPE is used in both datasets. This is interpreted as: the token is inside (I), in the beginning (B) of a following chunk of the same type or outside (O) of a chunk phrase. The named entities in the CoNLL-2003 dataset are annotated using four entity types, namely Location (LOC), Miscellaneous (MISC), Organization (ORG) and Person (PER). In Alan Ritter's corpus, entity types were not exactly the same, so they had to be converted, as we mention further on this section. The #MSM2013 corpus only contains annotated named entities and their types. To ease experimentation, this corpus was converted to the same format as the other two.</p><h2 id="evaluation">Evaluation</h2><p>The performance of a NLP tool in a certain task can be estimated by the quality of its predictions on the classification of unseen data. Predictions made are either considered Positive or Negative (under some category) and expected judgments are called True or False (again, under a certain category). The following are common metrics used to assess classification:</p><ul><li><p><strong>Precision:</strong> - The proportion of correctly classified instances (True Positives) among all the classified instances under a certain category (True Positives and False Positives).</p></li><li><p><strong>Recall:</strong> - The proportion of correctly classified instances (True Positives) under a certain category (True Positives and False Negatives).</p></li><li><p><strong>F-measure:</strong> - Combines precision and recall, and is computed as the harmonic mean between the two metrics.</p></li></ul><p>The previous metrics provide insights on the behavior of the tool. We can go further and compute the previous estimations in different ways such as:</p><ul><li><p><strong>Micro Averaging:</strong> - The entire text is treated as a single document and the individual correct classifications are summed up.</p></li><li><p><strong>Macro Averaging:</strong> - The precision and recall metrics are computed for each document and then averaged.</p></li></ul><h2 id="results-and-discussion">Results and discussion</h2><p>On the CoNLL dataset, which uses formal language, standard toolkits perform well. OpenNLP excels with F1=99% in tokenization, 88% in POS-tagging and 83% in chunking. In the NER task, NLTK~(89%) and OpenNLP~(88%) performed closely. TwitterNLP also performed well in this dataset. This is not that surprising if we add that the CoNLL-2003 dataset was one of the corpora TwitterNLP was trained on, and it is probably also tuned for this corpus.</p><p>As expected, the performance of standard toolkits, developed with formal text in mind, decreases when used in the social network corpora. This difference is between 5-8% for tokenization, 17% for POS-tagging, 17-40% for chunking, or 5-18% for NER. This is not the case of Pattern, which performs poorly in the CoNLL corpus but improves significantly when tokenizing, PoS tagging and chunking the Twitter corpora. Although not developed specifically for Twitter, OpeNLP and CoreNLP still obtain interesting results for tokenization and NER in its corpus (F1 &gt; 80%).</p><p>Also as expected, in the Twitter corpus, the Twitter-oriented toolkits performed better than the others. TweetNLP was the best in the tokenization (97%) and POS-tagging (95%) tasks. TwitterNLP performed closely (96% and 92%). In the case of TwitIE, the difference of performance in different types of text was not relevant. Once again, it should be highlighted that TwitterNLP was trained with the Twitter corpus, so this comparison is not completely fair.</p><p>Using only the available pre-trained models, there is not one toolkit that overperformed all the others in every scenario. Though, some are more balanced than the others. Even if it cannot be seen as a strong conclusion, the results suggest that OpenNLP is the best choice for news text, and TwitterNLP for social media text. Although the latter result was biased on the TWitter corpus, where TwitterNLP was trained on, we also tested it on another corpus, where it got the best results. It should be noticed that we ended up using datasets that were more appropriate for specific tasks. For instance, although its text of the CoNLL-2003 dataset is tokenized, POS-tagged, and chunked, it was specifically developed for a NER shared task. On the other hand, we did not use the CoNLL-2000, developed for a chunking shared task.</p><p>As expected, standard toolkits perform better in formal texts, while Twitter-oriented tools got better results in social media text. These results might be useful for potential users willing to select the most appropriate tools for their specific purposes, especially if they do not have time or expertise to train new models.</p><p>More details about this experiment, including the detailed results, can be found in the <a href="http://drops.dagstuhl.de/opus/volltexte/2016/6008/pdf/OASIcs-SLATE-2016-3.pdf">full text version</a>.</p>]]></content>
      
      
      
        <tags>
            
            <tag> nlp </tag>
            
            <tag> natural-language-processing </tag>
            
            <tag> research </tag>
            
            <tag> benchmark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Performance tips for Django applications</title>
      <link href="/2017/08/18/django-performance-tips/"/>
      <url>/2017/08/18/django-performance-tips/</url>
      
        <content type="html"><![CDATA[<p>Performance is an typical concern when developing applications. In order to have a good back-end performance it is important to be aware of memory footprints that our programs uses, CPU usage, database handling, etc. If no precaution measures are taken, these can quickly become bottlenecks and hurt the general performance of our system. This post shows a collection of performance tips that we can use in our Django application in order to save us from headaches in the future.</p><h2 id="use-.count-instead-of-len-in-querysets">Use .count() instead of len() in querysets</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">musics = Music.objects.<span class="built_in">all</span>()</span><br><span class="line">n_records = musics.count()</span><br></pre></td></tr></table></figure><p>Using the <em>.count()</em> is faster since it uses the <em>COUNT()</em> function at a database level. The <em>len()</em> method forces the queryset to be evaluated and retrieve results that you we will not use if all we want to do is count how many objects are there.</p><h2 id="use-a-combination-of-.filter-and-.exists-to-test-existence-and-membership">Use a combination of .filter() and .exists() to test existence and membership</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">musics = Music.objects.<span class="built_in">filter</span>(title=<span class="string">&#x27;Django rocks&#x27;</span>)</span><br><span class="line"><span class="keyword">if</span> musics.exists():</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure><p>Django provides an .exists() method that we can use instead of counting objects with <em>.count()</em> or testing entries for inclusion with <em>obj in queryset</em> .</p><h2 id="delay-queryset-evaluations">Delay queryset evaluations</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">musics = Music.objects.<span class="built_in">all</span>()</span><br><span class="line"> </span><br><span class="line"><span class="comment"># typical case of fetching all data from database into memory</span></span><br><span class="line"><span class="keyword">for</span> music <span class="keyword">in</span> musics:</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure><p>Django querysets are lazy, i.e, they are only evaluated (database hits) when strictly necessary, so we should create and combine querysets before performing certain operations such as iteration, <em>len()</em> or slicing which force the results to be fetched from the database. Trips to the database are more time consuming.</p><h2 id="avoid-caching-mechanisms-for-one-time-operations">Avoid caching mechanisms for one time operations</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">musics = Music.objects.<span class="built_in">all</span>()</span><br><span class="line"> </span><br><span class="line"><span class="comment"># avoids queryset caching</span></span><br><span class="line"><span class="keyword">for</span> music <span class="keyword">in</span> musics.iterator():</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure><p>The <em>.iterator()</em> method bypasses the internal caching mechanisms and might be useful if we know we are not going to use these objects anymore. Also, this largely reduces the memory footprint, which can be useful if we are loading millions of rows from the database.</p><h2 id="fetch-only-the-required-columns">Fetch only the required columns</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># returns a dict</span></span><br><span class="line">musics = Music.objects.values(<span class="string">&#x27;title&#x27;</span>,<span class="string">&#x27;album&#x27;</span>)</span><br></pre></td></tr></table></figure><p>or</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># returns a list</span></span><br><span class="line">musics = Music.objects.value_list(<span class="string">&#x27;title&#x27;</span>,<span class="string">&#x27;album&#x27;</span>)</span><br></pre></td></tr></table></figure><p>These methods avoid creating full model instances and retrieve only the desired field values, avoiding the extra work of fetching the extra columns.</p><h2 id="fetch-related-objects-in-a-single-batch">Fetch related objects in a single batch</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># fetches related many-to-many and many-to-one objects</span></span><br><span class="line">musics = Music.objects.<span class="built_in">all</span>().prefetch_related(<span class="string">&#x27;genres&#x27;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># fetches foreign key relations and one-to-one objects</span></span><br><span class="line">musics = Music.objects.<span class="built_in">all</span>().select_related(<span class="string">&#x27;author&#x27;</span>)</span><br></pre></td></tr></table></figure><p>These methods retrieve additional objects, to avoid fetching them later. This also caches all the results into memory, which may or not be desirable.</p><h2 id="page-results">Page results</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">musics = Music.objects.<span class="built_in">all</span>()</span><br><span class="line"></span><br><span class="line">paginator = Paginator(musics, per_page = <span class="number">2000</span>)</span><br><span class="line"><span class="keyword">for</span> page <span class="keyword">in</span> paginator.page_range:</span><br><span class="line">    <span class="keyword">for</span> obj <span class="keyword">in</span> paginator.page(page).object_list:</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure><p>Pagination avoids loading all the objects into memory. This will drastically reduce the memory usage since it fetches slices of our dataset, one chunk of rows at a time, from the database.</p><h2 id="use-bulk_create-to-insert-a-batch-of-records">Use bulk_create() to insert a batch of records</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Music.objects.bulk_create(musics)</span><br></pre></td></tr></table></figure><p>Each time we call the <em>.save()</em> method on a model instance, a round trip to the database is performed. Besides, signals are sent for each save operation. This can quickly bring an huge overhead when dealing with thousands or millions of records. A possible workaround is to use the <em>bulk_create</em> method which inserts records in a single query. We only need to give the list of objects we wish to write back to disk in a single database round trip. However, it is important to note that custom <em>save()</em> methods and signals will not be called.</p><h2 id="use-distributed-and-asynchronous-processing">Use distributed and asynchronous processing</h2><p>External concurrency libraries such as <a href="http://www.tornadoweb.org/en/stable/">Tornado</a>, <a href="https://twistedmatrix.com/trac/">Twisted</a> or <a href="https://docs.python.org/3/library/asyncio.html">Asyncio</a> provide non-blocking behavior and asynchronous I/O, great for performing I/O bound tasks such as reading and writing to disk/network. <a href="http://www.celeryproject.org/">Celery</a> is also great to perform distributed and CPU bound background tasks.</p><p>Here is an example of an hypothetical processing of electronic consumption bills, using the backport version of asyncio for Python 2.7.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> django.conf <span class="keyword">import</span> settings</span><br><span class="line"><span class="keyword">from</span> django.core.management.base <span class="keyword">import</span> BaseCommand, CommandError</span><br><span class="line"><span class="keyword">from</span> django.db <span class="keyword">import</span> transaction</span><br><span class="line"><span class="keyword">from</span> consumption.models <span class="keyword">import</span> ElectricConsumption</span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> trollius <span class="keyword">as</span> asyncio</span><br><span class="line"><span class="keyword">from</span> trollius <span class="keyword">import</span> From</span><br><span class="line"> </span><br><span class="line">COUNTER = <span class="number">0</span></span><br><span class="line"> </span><br><span class="line"><span class="meta">@asyncio.coroutine</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process_client</span>(<span class="params">all_events, client_number, total</span>):</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        client_events = all_events.<span class="built_in">filter</span>(client_number=client_number).order_by(<span class="string">&#x27;timestamp&#x27;</span>)</span><br><span class="line"> </span><br><span class="line">        <span class="built_in">print</span> <span class="string">&quot;Scheduled processing of client &#123;&#125;&quot;</span>.<span class="built_in">format</span>(client_number)</span><br><span class="line"> </span><br><span class="line">        <span class="comment">#specify that a context switch can happen here</span></span><br><span class="line">        <span class="keyword">yield</span> From(asyncio.sleep(<span class="number">1</span>))</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># Compute bills (I/O bound task)</span></span><br><span class="line">        ElectricConsumption.set_bill(client_events)</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">global</span> COUNTER</span><br><span class="line">        COUNTER += <span class="number">1</span></span><br><span class="line">        <span class="built_in">print</span> <span class="string">&quot;Finished processing client &#123;&#125;: &quot;</span> \</span><br><span class="line">              <span class="string">&quot;(&#123;&#125; of &#123;&#125; = &#123;&#125; %))&quot;</span>.<span class="built_in">format</span>(client_number,</span><br><span class="line">                                          COUNTER, total,</span><br><span class="line">                                          <span class="built_in">str</span>(COUNTER / <span class="built_in">float</span>(total) * <span class="number">100</span>))</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="built_in">print</span> <span class="string">&quot;Error processing client &#123;&#125;)&quot;</span>.<span class="built_in">format</span>(client_number)</span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Command</span>(<span class="params">BaseCommand</span>):</span></span><br><span class="line">    <span class="built_in">help</span> = <span class="string">&#x27;Process electric consumptions&#x27;</span></span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Process clients</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">handle</span>(<span class="params">self, *args, **options</span>):</span></span><br><span class="line">        all_events = ElectricConsumption.objects.<span class="built_in">all</span>()</span><br><span class="line">        clients = all_events.values(<span class="string">&#x27;client_number&#x27;</span>)</span><br><span class="line">        total = clients.count()</span><br><span class="line"></span><br><span class="line">        <span class="comment">#gets the event loop that manages the execution flow of our future tasks</span></span><br><span class="line">        loop = asyncio.get_event_loop()</span><br><span class="line">        tasks = []</span><br><span class="line">        <span class="keyword">with</span> transaction.atomic():</span><br><span class="line">            <span class="keyword">for</span> client <span class="keyword">in</span> clients:</span><br><span class="line">                client_number = client[<span class="string">&#x27;client_number&#x27;</span>]</span><br><span class="line"> </span><br><span class="line">                <span class="comment">#register our asynchronous tasks</span></span><br><span class="line">                tasks += [asyncio.ensure_future(process_client(all_events, client_number, total))]</span><br><span class="line"></span><br><span class="line">            loop.run_until_complete(asyncio.wait(tasks))</span><br><span class="line">            loop.close()</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ python manage.py electric_bills.py</span><br><span class="line"> </span><br><span class="line">Scheduled processing of client 4521</span><br><span class="line">Scheduled processing of client 4174</span><br><span class="line">Scheduled processing of client 7531</span><br><span class="line">Scheduled processing of client 7584</span><br><span class="line">Scheduled processing of client 7419</span><br><span class="line">Finished processing client 7584: (1 of 5 = 20.0 %))</span><br><span class="line">Finished processing client 4174: (2 of 5 = 40.0 %))</span><br><span class="line">Finished processing client 7531: (3 of 5 = 60.0 %))</span><br><span class="line">Finished processing client 7419: (4 of 5 = 80.0 %))</span><br><span class="line">Finished processing client 4521: (5 of 5 = 100.0 %))</span><br></pre></td></tr></table></figure><p>These small tips make a noticeably difference when dealing with huge datasets and are good investments, regarding performance, in the long term.</p>]]></content>
      
      
      
        <tags>
            
            <tag> django </tag>
            
            <tag> python </tag>
            
            <tag> performance </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Populating a PostgreSQL database</title>
      <link href="/2017/08/17/populating-postgres/"/>
      <url>/2017/08/17/populating-postgres/</url>
      
        <content type="html"><![CDATA[<p>It is often the case that we need to populate a table with initial data. A typical approach is to run an sql script to perform a bulk insert. However this is not ideal for cases where there are millions of rows. To tackle this, PostgreSQL provides the COPY command, a very efficient way of inserting a large amount of data.</p><p>This command may be used like this:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">COPY table_name(column1, column2, column3, ...) FROM <span class="string">&#x27;data.csv&#x27;</span> DELIMITER <span class="string">&#x27;;&#x27;</span> CSV HEADER;</span><br></pre></td></tr></table></figure><p>where we specify the table name, its columns and the data file.</p><p>To illustrate, let's create a table that mimics an household electric power consumption. We can use a pre-built docker image containing a PostgreSQL installation for this and a <a href="https://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption">public dataset</a> for testing.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">$ cat docker-compose</span><br><span class="line"></span><br><span class="line">version: <span class="string">&#x27;2&#x27;</span></span><br><span class="line"> </span><br><span class="line">services:</span><br><span class="line">  db:</span><br><span class="line">    image: postgres</span><br><span class="line">    ports:</span><br><span class="line">      - <span class="string">&quot;5432:5432&quot;</span></span><br><span class="line">    volumes:</span><br><span class="line">      - data:/var/lib/postgresql/data</span><br><span class="line">    environment:</span><br><span class="line">      POSTGRES_USER: postgres</span><br><span class="line">      POSTGRES_PASSWORD: postgres</span><br><span class="line"> </span><br><span class="line">volumes:</span><br><span class="line">  data:</span><br></pre></td></tr></table></figure><p>We are now ready to configure our database:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">$ docker-compose up -d db</span><br><span class="line">$ docker cp household_power_consumption.csv db_1:/var/lib/postgresql/data/household_power_consumption.csv</span><br><span class="line">$ docker-compose run --rm db psql -h db -p 5432 -U postgres --password</span><br><span class="line">  </span><br><span class="line">CREATE TABLE power_consumption (</span><br><span class="line">    Date character varying(50),</span><br><span class="line">    Time character varying(50),</span><br><span class="line">    Global_active_power character varying(50),</span><br><span class="line">    Global_reactive_power character varying(50),</span><br><span class="line">    Voltage character varying(50),</span><br><span class="line">    Global_intensity character varying(50),</span><br><span class="line">    Sub_metering_1 character varying(50),</span><br><span class="line">    Sub_metering_2 character varying(50),</span><br><span class="line">    Sub_metering_3 character varying(50)</span><br><span class="line">);</span><br></pre></td></tr></table></figure><p>Finally, we can load the data into our newly created table: <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">\timing on</span><br><span class="line"> </span><br><span class="line">COPY power_consumption(Date,Time,Global_active_power,Global_reactive_power,Voltage,Global_intensity,Sub_metering_1,Sub_metering_2,Sub_metering_3) FROM <span class="string">&#x27;/var/lib/postgresql/data/household_power_consumption.csv&#x27;</span> DELIMITER <span class="string">&#x27;;&#x27;</span> CSV HEADER;</span><br><span class="line"></span><br><span class="line">Time: 7906.845 ms</span><br></pre></td></tr></table></figure></p><p>It took roughly 8 seconds to insert 2 075 259 records. This operation works best if there are not any indexes or foreign keys present that may introduce overhead in each insert. It is usually preferred to create them after.</p>]]></content>
      
      
      
        <tags>
            
            <tag> performance </tag>
            
            <tag> database </tag>
            
            <tag> sql </tag>
            
            <tag> postgres </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Reclaiming disk space from docker</title>
      <link href="/2017/08/15/docker-cleanup/"/>
      <url>/2017/08/15/docker-cleanup/</url>
      
        <content type="html"><![CDATA[<p>Docker is a great tool to create isolated micro services and environments. However, over time the number of intermediate image layers, size of log files used by the containers and volumes increases which can take up considerable disk space. Besides, dangling images and stopped containers which we do not use anymore are not automatically removed. Fortunately, docker has some options which can help us reclaim disk space.</p><p>Dangling images are not referenced by any other image, so they can be garbage collected.</p><p><strong>List dangling images</strong> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker images -f dangling=<span class="literal">true</span></span><br></pre></td></tr></table></figure> <strong>Removing dangling images</strong> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker rmi $(docker images -q -f dangling=<span class="literal">true</span>)</span><br></pre></td></tr></table></figure></p><p>Similarly, we can list all stopped containers and remove them.</p><p><strong>List stopped containers</strong> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker ps -a -f status=exited</span><br></pre></td></tr></table></figure> <strong>Removing stopped containers</strong> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker rm $(docker ps -a -q -f status=exited)</span><br></pre></td></tr></table></figure></p><p>We can also remove data volumes from containers that do not exist anymore, i.e., dangling volumes that are not automatically removed.</p><p><strong>List dangling volumes</strong> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker volume ls -f dangling=<span class="literal">true</span></span><br></pre></td></tr></table></figure> <strong>Removing dangling volumes</strong> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker volume rm $(docker volume ls -q -f dangling=<span class="literal">true</span>)</span><br></pre></td></tr></table></figure></p><p>Another good practice to minimize disk space usage is to periodically rotate the docker output logs. We can use the built-in utility logrotate by simply creating a configuration file in /etc/logrotate.d/ .</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ vim /etc/logrotate.d/docker-log</span><br><span class="line"> </span><br><span class="line">/var/lib/docker/containers/*/*.<span class="built_in">log</span> &#123;</span><br><span class="line">  rotate 10</span><br><span class="line">  daily</span><br><span class="line">  compress</span><br><span class="line">  size=50M</span><br><span class="line">  missingok</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The above configuration archives and compress the logs when they reach a size of 50M. This rotation is performed daily and after 10 archived logs, the oldest one is removed.</p><p>It is important to be aware of the disk space consumed by docker and perform some cleaning tasks once in a while or we might end up with some space issues. Docker also offers a great <a href="https://docs.docker.com/">documentation</a> where we can further inspect its options.</p>]]></content>
      
      
      
        <tags>
            
            <tag> memory </tag>
            
            <tag> docker </tag>
            
            <tag> disk </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Reattaching shell sessions</title>
      <link href="/2017/08/15/reattaching-sessions/"/>
      <url>/2017/08/15/reattaching-sessions/</url>
      
        <content type="html"><![CDATA[<p>Ever started a shell session running a process, quickly realizing that will take longer than you expected to finish? For instance, a remote ssh session where you do not want to leave your computer turned on just to keep the session open. Fortunately, there is a workaround by using a combination of <a href="https://www.gnu.org/software/screen/">screen</a> and <a href="https://github.com/nelhage/reptyr">reptyr</a>.</p><p>For example, imagine that we are running this hypothetic slow process that prints in each second the next number of the Fibonacci sequence:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fibo</span>():</span></span><br><span class="line">prev = <span class="built_in">sum</span> = <span class="number">0</span></span><br><span class="line">actual = <span class="number">1</span></span><br><span class="line">        <span class="built_in">print</span> actual</span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line"><span class="built_in">next</span> = prev + actual</span><br><span class="line">prev = actual</span><br><span class="line">actual = <span class="built_in">next</span></span><br><span class="line"><span class="built_in">print</span> actual</span><br><span class="line">time.sleep(<span class="number">1</span>)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    fibo()</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">$ python fibo.py</span><br><span class="line"> </span><br><span class="line">1</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">5</span><br><span class="line">8</span><br><span class="line">13</span><br><span class="line">21</span><br><span class="line">34</span><br><span class="line">55</span><br><span class="line">89</span><br><span class="line">144</span><br><span class="line">233</span><br><span class="line">377</span><br><span class="line">610</span><br><span class="line">987</span><br><span class="line">1597</span><br><span class="line">2584</span><br><span class="line">4181</span><br><span class="line">6765</span><br><span class="line">10946</span><br><span class="line">17711</span><br><span class="line">28657</span><br></pre></td></tr></table></figure><p>We are now interested in moving this job to another session where we can attach and detach any time, while leaving the job running. In order to achieve this, we start by creating a new screen session in a new window: <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ screen -S fibonacci</span><br></pre></td></tr></table></figure></p><p>The screen tool lets us create multiple windows with shells inside. We can also leave those windows at anytime and reenter them later, always leaving their internal programs running. Once we have this new screen session, we can move our running job to this new shell:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">$ ps -u | grep fibo.py</span><br><span class="line"></span><br><span class="line">alexpnt  10776  0.0  0.1  25540  6204 pts/4    S+   14:00   0:00 python fibo.py</span><br><span class="line"> </span><br><span class="line">$ sudo -i</span><br><span class="line">$ <span class="built_in">echo</span> 0 &gt; /proc/sys/kernel/yama/ptrace_scope</span><br><span class="line">$ <span class="built_in">exit</span> </span><br><span class="line">$ reptyr 10776</span><br><span class="line"></span><br><span class="line">46368</span><br><span class="line">75025</span><br><span class="line">121393</span><br><span class="line">196418</span><br><span class="line">317811</span><br><span class="line">514229</span><br><span class="line">832040</span><br><span class="line">1346269</span><br><span class="line">2178309</span><br><span class="line">3524578</span><br><span class="line">5702887</span><br><span class="line">9227465</span><br><span class="line">14930352</span><br></pre></td></tr></table></figure><p>We temporarily enable the ability to attach programs, as root. Then, we grab the process id and pass it to the reptyr tool, which does a great job in moving our old process back to this new shell. We can now detach from this screen session by typing <em>Ctrl + Shift + D</em> and confirming the session is saved:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ screen -ls</span><br><span class="line"></span><br><span class="line">There is a screen on:</span><br><span class="line">10822.fibo(Detached)</span><br><span class="line">1 Socket <span class="keyword">in</span> /run/screens/S-alexpnt.</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>Later, when can reattach to the screen:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">$ screen -r fibo</span><br><span class="line"> </span><br><span class="line">24157817</span><br><span class="line">39088169</span><br><span class="line">63245986</span><br><span class="line">102334155</span><br><span class="line">165580141</span><br><span class="line">267914296</span><br><span class="line">433494437</span><br><span class="line">701408733</span><br><span class="line">1134903170</span><br><span class="line">1836311903</span><br><span class="line">2971215073</span><br><span class="line">4807526976</span><br><span class="line">7778742049</span><br><span class="line">12586269025</span><br></pre></td></tr></table></figure><p>PS: You might need to run <code>bash echo 0 &gt; /proc/sys/kernel/yama/ptrace_scope</code></p><p>This is a very simple example with a trivial job running just to illustrate the idea of moving random jobs to new screen sessions. We can avoid this by always starting long running process in new screen sessions. In case we forget, we can use this process to rescue us.</p>]]></content>
      
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> shell </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Free up memory by cleaning caches</title>
      <link href="/2017/08/12/clear-cache/"/>
      <url>/2017/08/12/clear-cache/</url>
      
        <content type="html"><![CDATA[<p>Running out of memory can be an issue when you are running a lot services on your development box, such as web services, databases, editors, etc. Over time, the caching system eats up a good portion of your memory resources. Event though Linux has a good memory management system, the kernel also has a useful mechanism to free up caches.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sync; <span class="built_in">echo</span> 3 &gt; /proc/sys/vm/drop_caches</span><br></pre></td></tr></table></figure><p>Basically, the initial <em>sync</em> command writes pending data in memory back to disk. After this synchronization, we can release memory by dropping cached objects. The value '3' is just an internal argument used to specify the type of caches that will be freed. The writing event triggers the cleaning task. You can see its effects by checking your memory in real time:</p><p>Before: <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">watch free -h -t</span><br><span class="line">              total        used        free      shared  buff/cache   available</span><br><span class="line">Mem:           3.8G        2.0G        1.1G        134M        622M        1.4G</span><br><span class="line">Swap:          2.0G        531M        1.5G</span><br><span class="line">Total:         5.8G        2.5G        2.6G</span><br></pre></td></tr></table></figure></p><p>After: <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">watch free -h -t</span><br><span class="line">              total        used        free      shared  buff/cache   available</span><br><span class="line">Mem:           3.8G        2.0G        1.4G        133M        375M        1.4G</span><br><span class="line">Swap:          2.0G        532M        1.5G</span><br><span class="line">Total:         5.8G        2.5G        2.9G</span><br></pre></td></tr></table></figure></p><p>It is important to note that this will cause objects to be recreated later, which may lead to some intense CPU/IO usage. However, it is still an approach you can use if you feel you are running out of memory and want to reclaim back some portions of it.</p>]]></content>
      
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> unix </tag>
            
            <tag> memory </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Microservices, Docker and Django - Part II</title>
      <link href="/2017/08/06/dockerizing-your-django-app-2/"/>
      <url>/2017/08/06/dockerizing-your-django-app-2/</url>
      
        <content type="html"><![CDATA[<p>A typical web application has a set of components that work together such as the application backend, the frontend, the database or other components such as cache services, proxy servers, etc. Docker is a great tool to decouple our different services and isolate them. In this post we will decouple our web application into two services, the application and the database.</p><p>We start by getting a copy of our code base:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ git <span class="built_in">clone</span> -b postgres https://github.com/AlexPnt/MusicWallet.git</span><br><span class="line">$ <span class="built_in">cd</span> MusicWallet</span><br></pre></td></tr></table></figure><p>Our project structure is as follows:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">└── MusicWallet</span><br><span class="line">    ├── docker-compose.yml</span><br><span class="line">    ├── musicwalletproject</span><br><span class="line">    │   ├── docker-entrypoint.sh</span><br><span class="line">    │   ├── Dockerfile</span><br><span class="line">    │   ├── manage.py</span><br><span class="line">    │   ├── musicwalletapp</span><br><span class="line">    │   ├── musicwalletproject</span><br><span class="line">    │   └── requirements.txt</span><br><span class="line">    └── README.md</span><br></pre></td></tr></table></figure><p>We declare our two services in a configuration file called <em>docker-compose.yml</em> .</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">version: <span class="string">&#x27;2&#x27;</span></span><br><span class="line"></span><br><span class="line">services:</span><br><span class="line">  db:</span><br><span class="line">    image: postgres</span><br><span class="line">    restart: always</span><br><span class="line">    ports:</span><br><span class="line">      - <span class="string">&quot;5432:5432&quot;</span></span><br><span class="line">    volumes:</span><br><span class="line">      - data:/var/lib/postgresql/data</span><br><span class="line">    environment:</span><br><span class="line">      POSTGRES_USER: postgres</span><br><span class="line">      POSTGRES_PASSWORD: postgres</span><br><span class="line"></span><br><span class="line">  web:</span><br><span class="line">    restart: always</span><br><span class="line">    build: ./musicwalletproject</span><br><span class="line">    <span class="built_in">command</span>: python manage.py runserver 0.0.0.0:8000 --settings=musicwalletproject.settings.development</span><br><span class="line">    volumes:</span><br><span class="line">      - ./musicwalletproject:/code</span><br><span class="line">    ports:</span><br><span class="line">      - <span class="string">&quot;8000:8000&quot;</span></span><br><span class="line">    depends_on:</span><br><span class="line">      - db</span><br><span class="line"></span><br><span class="line">volumes:</span><br><span class="line">  data:</span><br></pre></td></tr></table></figure><p>We declare our database service 'db' and our web application 'web'. The database service pulls a docker image from the <a href="https://hub.docker.com/_/postgres/">official repository</a>, setups up some credentials and expose the standard ports. The service 'web' uses our code base to build an image that will serve requests on port 8000.</p><p>Our 'web' service used a file called <em>Dockerfile</em> to setup the docker image:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">FROM python:2</span><br><span class="line"> </span><br><span class="line">ENV PYTHONUNBUFFERED 1</span><br><span class="line"> </span><br><span class="line">RUN mkdir /code</span><br><span class="line">WORKDIR /code</span><br><span class="line">ADD . /code</span><br><span class="line"> </span><br><span class="line">ADD requirements.txt /code/</span><br><span class="line">RUN pip install -r /code/requirements.txt</span><br><span class="line"> </span><br><span class="line">COPY ./docker-entrypoint.sh /</span><br><span class="line"> </span><br><span class="line">ENTRYPOINT [<span class="string">&quot;/docker-entrypoint.sh&quot;</span>]</span><br></pre></td></tr></table></figure><p>This file simply instructs docker to copy our code base into the container and install all the required dependencies. Besides, it also runs some setup commands specified in <em>docker-entrypoint.sh</em>, which applies migrations to the database, copy the static files to a single location and starts the web server:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/sh</span></span><br><span class="line"> </span><br><span class="line">python manage.py makemigrations --settings=musicwalletproject.settings.development</span><br><span class="line">python manage.py migrate --settings=musicwalletproject.settings.development</span><br><span class="line">python manage.py collectstatic --settings=musicwalletproject.settings.development --noinput</span><br><span class="line">python manage.py runserver 0.0.0.0:8000 --settings=musicwalletproject.settings.development</span><br></pre></td></tr></table></figure><p>We are now ready to launch our services. Starting with the database service, with these docker commands we can quickly launch our new database container and create our new database:<br /><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ docker-compose up db</span><br><span class="line">$ docker-compose run --rm db psql -h db -U postgres -c <span class="string">&quot;CREATE DATABASE musicwallet_db&quot;</span> </span><br></pre></td></tr></table></figure></p><p>Similarly, we build our web application and launch it: <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ docker-compose build web</span><br><span class="line">$ docker-compose up web</span><br></pre></td></tr></table></figure></p><p>If we want to inspect if our services are running, we can use the docker-compose command <em>ps</em>: <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ docker-compose ps</span><br><span class="line">  </span><br><span class="line">      Name                     Command               State           Ports          </span><br><span class="line">-----------------------------------------------------------------------------------</span><br><span class="line">musicwallet_db_1    docker-entrypoint.sh postgres    Up      0.0.0.0:5432-&gt;5432/tcp </span><br><span class="line">musicwallet_web_1   /docker-entrypoint.sh pyth ...   Up      0.0.0.0:8000-&gt;8000/tcp </span><br></pre></td></tr></table></figure></p><p>It seems to be working. We can further inspect the logs to see if any error occurred during launch:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">$ docker-compose up</span><br><span class="line"> </span><br><span class="line">Starting musicwallet_db_1</span><br><span class="line">Starting musicwallet_web_1</span><br><span class="line">Attaching to musicwallet_db_1, musicwallet_web_1</span><br><span class="line">db_1   | LOG:  database system was interrupted; last known up at 2017-08-09 23:44:42 UTC</span><br><span class="line">db_1   | LOG:  database system was not properly shut down; automatic recovery <span class="keyword">in</span> progress</span><br><span class="line">db_1   | LOG:  invalid record length at 0/15975C0: wanted 24, got 0</span><br><span class="line">db_1   | LOG:  redo is not required</span><br><span class="line">db_1   | LOG:  MultiXact member wraparound protections are now enabled</span><br><span class="line">db_1   | LOG:  autovacuum launcher started</span><br><span class="line">db_1   | LOG:  database system is ready to accept connections</span><br><span class="line">web_1  | No changes detected</span><br><span class="line">web_1  | Operations to perform:</span><br><span class="line">web_1  |   Apply all migrations: admin, auth, authtoken, contenttypes, musicwalletapp, sessions</span><br><span class="line">web_1  | Running migrations:</span><br><span class="line">web_1  |   No migrations to apply.</span><br><span class="line">web_1  | </span><br><span class="line">web_1  | 0 static files copied to <span class="string">&#x27;/code/musicwalletproject/settings/static&#x27;</span>, 93 unmodified.</span><br><span class="line">web_1  | Performing system checks...</span><br><span class="line">web_1  | </span><br><span class="line">web_1  | System check identified no issues (0 silenced).</span><br><span class="line">web_1  | August 09, 2017 - 23:55:50</span><br><span class="line">web_1  | Django version 1.10.2, using settings <span class="string">&#x27;musicwalletproject.settings.development&#x27;</span></span><br><span class="line">web_1  | Starting development server at http://0.0.0.0:8000/</span><br><span class="line">web_1  | Quit the server with CONTROL-C.</span><br></pre></td></tr></table></figure><p>Finally, we can head over to <a href="localhost:8000">localhost:8000</a>:</p><p><img src="/images/musicwallet.png" /></p><p>The web application is ready to be used ! You can live test it <a href="http://musicwallet.pythonanywhere.com/">here.</a></p><p>Alternatively, you can use perform REST calls using the <a href="https://httpie.org/">httpie</a> client.</p><p>Obtaining the authentication token:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ http --json POST localhost:8000/api-token-auth/ username=myuser password=mypassword</span><br></pre></td></tr></table></figure><p>List users: <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ http GET localhost:8000/api/users/ Authorization:<span class="string">&quot;Token &lt;token_id&gt;&quot;</span></span><br></pre></td></tr></table></figure> Get details from a user <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ http GET localhost:8000/api/users/&lt;id&gt;/ Authorization:<span class="string">&quot;Token &lt;token_id&gt;&quot;</span></span><br></pre></td></tr></table></figure> Edit an existing user by sending a request with json data in the body of the request. <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ http --json PUT localhost:8000/api/users/&lt;id&gt;/ username=newname email=newemail password=newpassword Authorization:<span class="string">&quot;Token &lt;token_id&gt;&quot;</span></span><br></pre></td></tr></table></figure> Delete an existing user. <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ http DELETE localhost:8000/api/users/&lt;id&gt;/ Authorization:<span class="string">&quot;Token &lt;token_id&gt;&quot;</span></span><br></pre></td></tr></table></figure> List musics: <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ http GET localhost:8000/api/musics/ Authorization:<span class="string">&quot;Token &lt;token_id&gt;&quot;</span></span><br></pre></td></tr></table></figure> Get details from a music. <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ http GET localhost:8000/api/musics/&lt;id&gt;/ Authorization:<span class="string">&quot;Token &lt;token_id&gt;&quot;</span></span><br></pre></td></tr></table></figure> Create a new music by sending a request with json data in the body of the request. <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ http --json POST localhost:8000/api/musics/ title=mytitle artist=myartist album=myalbum Authorization:<span class="string">&quot;Token &lt;token_id&gt;&quot;</span></span><br></pre></td></tr></table></figure> Edit an existing music by sending a request with json data in the body of the request. <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ http --json PUT localhost:8000/api/musics/&lt;id&gt;/ title=newtitle artist=newartist album=newalbum Authorization:<span class="string">&quot;Token &lt;token_id&gt;&quot;</span></span><br></pre></td></tr></table></figure> Delete an existing music. <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ http DELETE localhost:8000/api/musics/&lt;id&gt;/ Authorization:<span class="string">&quot;Token &lt;token_id&gt;&quot;</span></span><br></pre></td></tr></table></figure> Add an existing music to the list of favourites from an existing user. <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ http POST localhost:8000/api/users/&lt;music_id&gt;/add_fav_music/ Authorization:<span class="string">&quot;Token &lt;token_id&gt;&quot;</span></span><br></pre></td></tr></table></figure> Delete a favourite music from the list of favourites from an existing user. <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ http DELETE localhost:8000/api/users/&lt;music_id&gt;/remove_fav_music/ Authorization:<span class="string">&quot;Token &lt;token_id&gt;&quot;</span></span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      
        <tags>
            
            <tag> django </tag>
            
            <tag> development </tag>
            
            <tag> python </tag>
            
            <tag> docker </tag>
            
            <tag> microservices </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Microservices, Docker and Django - Part I</title>
      <link href="/2017/07/31/dockerizing-your-django-app/"/>
      <url>/2017/07/31/dockerizing-your-django-app/</url>
      
        <content type="html"><![CDATA[<h2 id="monolithic-vs-microservices">Monolithic vs Microservices</h2><p>Over the past years, there has been a paradigm shift in the architecture of software, i.e., the microservices approach to software development is taking over the traditional monolithic architecture.</p><p>Monolithic applications are systems built as a single tightly coupled unit, which gets more complex as the application evolves. Monolithic applications share the same resources, libraries and memory, which brings faster accesses and inter service communications, however, it is harder to maintain, challenging to scale and more difficult to isolate services.</p><p>On the other hand, microservices are independent and isolated services with specific responsibilities, who communicate with each other. This brings an overall better organization, separation of concerns and performance in the long run. With microservices is easier to scale since we only need to 'upgrade' certain services and not the entire system. It is easier to deploy (we only update parts of the system) and more fault tolerant (a failure of one part does not bring down the entire system).</p><h2 id="docker-containerization">Docker Containerization</h2><p>Containers are a way of isolating services, just like virtual machines but with some important differences. While virtual machines create an isolated full stack, from the host operating system to the user application, containers share the host operating system, adding only the necessary libraries and applications, creating a lightweight sandbox on our system, with its own layered filesystem and network management. <a href="https://www.docker.com/">Docker</a> is a popular container system which uses Linux containers(LXC) under the hood, a native feature of Linux systems, in order to provide isolation of microservices.</p><p>With docker we can quickly change to different environment setups,</p><h2 id="installing-docker">Installing Docker</h2><p>To install docker on a Debian based box, we simply run these commands.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install \</span><br><span class="line">    apt-transport-https \</span><br><span class="line">    ca-certificates \</span><br><span class="line">    curl \</span><br><span class="line">    software-properties-common</span><br><span class="line"> </span><br><span class="line">$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -</span><br><span class="line"> </span><br><span class="line">$ sudo add-apt-repository \</span><br><span class="line">   <span class="string">&quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu \</span></span><br><span class="line"><span class="string">   <span class="subst">$(lsb_release -cs)</span> \</span></span><br><span class="line"><span class="string">   stable&quot;</span></span><br><span class="line"> </span><br><span class="line">$ sudo apt-get update</span><br><span class="line">$ sudo apt-get install docker-ce</span><br><span class="line">$ sudo groupadd docker</span><br></pre></td></tr></table></figure><h2 id="installing-docker-compose">Installing Docker Compose</h2><p>Docker Compose is a useful tool that let us configure and run multiple containers.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ pip install docker-compose</span><br></pre></td></tr></table></figure><h2 id="downloading-our-example-application-that-we-will-dockerize">Downloading our example application that we will dockerize</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ mkdir dockerize-musicwallet</span><br><span class="line">$ <span class="built_in">cd</span> dockerize-musicwallet</span><br><span class="line">$ git <span class="built_in">clone</span> -b postgres https://github.com/AlexPnt/MusicWallet.git</span><br></pre></td></tr></table></figure><p>We now have a code base from a <a href="https://github.com/AlexPnt/MusicWallet">web application</a> that let us manage users and their favourite musics, with a REST API, built with Django and the Django Rest Framework.</p><p>In the <a href="/2017/08/06/dockerizing-your-django-app-2/" title="Microservices, Docker and Django - Part II">Microservices, Docker and Django - Part II</a>, we will turn this application into a docker container.</p>]]></content>
      
      
      
        <tags>
            
            <tag> django </tag>
            
            <tag> development </tag>
            
            <tag> python </tag>
            
            <tag> docker </tag>
            
            <tag> microservices </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Mouting CIFS shares on a Linux/UNIX box</title>
      <link href="/2017/07/23/mount-cifs-shares/"/>
      <url>/2017/07/23/mount-cifs-shares/</url>
      
        <content type="html"><![CDATA[<p>Occasionally, there is the need to access files from a remote host which uses a diferent operating system or is located in a different network. The CIFS (Common Internet File System) is a sharing protocol that lets users share files across different systems.</p><p>On a Linux/UNIX box we can mount our shared folder as follows:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo mount -t cifs //remoteserver/shared /mnt/shared/ -o username=myusername,password=mypassword --verbose</span><br></pre></td></tr></table></figure><p>where:</p><ul><li><p><em>-t cifs</em> is the filesystem type (cifs in our case)</p></li><li><p><em>//remoteserver/shared</em> is our remote host ('shared' is the shared folder)</p></li><li><p><em>/mnt/shared/</em> is the local folder that will be mounted</p></li><li><p><em>-o username=myusername,password=mypassword</em> are the authentication credentials</p></li></ul><p><strong>Disclaimer:</strong> You might need to install CIFS utilities before attempting to run the above command.</p>]]></content>
      
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> cifs </tag>
            
            <tag> windows </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Interview - The Setup</title>
      <link href="/2017/07/22/the-setup/"/>
      <url>/2017/07/22/the-setup/</url>
      
        <content type="html"><![CDATA[<p>As a developer, one of the first steps before attempting to do any work is to create a good development environment. This involves choosing a set of tools that we will spend most of our time using. Therefore, I compiled a list of the tools that I use or have used on my daily routine.</p><p>Regarding operating systems, I started out with the initial Windows versions, but moved on to the GNU/Linux ecosystem. I have distro-hopped between some distributions such as <a href="https://www.ubuntu.com/">Ubuntu</a>, <a href="https://xubuntu.org/">Xubuntu</a>, <a href="https://www.debian.org/">Debian</a>, <a href="https://www.bunsenlabs.org/">CrunchBang</a> (now called BunsenLabs) and <a href="https://www.archlinux.org/">ArchLinux</a>. I have settled with the <a href="https://antergos.com/">Antergos</a> distro, which is an ArchLinux based distribution. So far, it has not given me much trouble and I can get the cool features expected from a rolling release distribution, i.e., a distro with constant, small and frequent updates, with a great level of customization and lightweight on system resources. My current setup includes the light Xfce Desktop environment and the fancy Numix theme suit. This choice reflects my preference for choosing functionality over aesthetics and graphics. For mobile devices, I stick with the Android ecosystem.</p><p>I use <a href="https://launchpad.net/terminator">Terminator</a> as my terminal of choice and Bash as the command-line shell. Filezila is my choice for downloading files over SFTP from remote servers. In terms of editing, I use the <a href="https://www.sublimetext.com/">Sublime Text</a> editor for simple and quick edits. For general development, I trust in the amazing Jetbrains development tools. My most used tools are <a href="https://www.jetbrains.com/pycharm">PyCharm</a>/<a href="https://www.jetbrains.com/idea">IntelliJ</a>, for server-side languages such as Python or Java, <a href="https://www.jetbrains.com/webstorm/">WebStorm</a> for dealing with web technologies and <a href="https://www.jetbrains.com/datagrip/">DataGrip</a> for dealing with databases. In the past I used other IDES, suchas Eclipse, Netbeans or CodeBlocks. My control version of choice is <a href="https://git-scm.com/">git</a>. If I need to edit LaTeX files, I use the <a href="https://github.com/alexandervdm/gummi">gummi</a> editor. For system resources monitoring, I use <a href="https://github.com/brndnmtthws/conky">conky</a>, with a custom configuration.</p><p>My browsing experience takes a big chunk of time. <a href="https://www.mozilla.org/en-GB/firefox/new/">Firefox</a> and <a href="https://www.chromium.org/getting-involved/download-chromium">Chromium</a> are my web browsers of choice. For Firefox, I make use of great add-ons such as <a href="https://github.com/buunguyen/octotree">Octotree</a> which lets me visualize the source code tree of Github and Gitlab projects and <a href="https://userstyles.org/">Stylish</a> to rewrite the appearance of some websites. <a href="http://chrispederick.com/work/web-developer/">WebDeveloper</a> and <a href="developer.yahoo.com/yslow">YSlow</a> are also great tools that help your web development.</p>]]></content>
      
      
      
        <tags>
            
            <tag> setup </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Complex lookups with Q objects in Django</title>
      <link href="/2017/07/19/q-objects/"/>
      <url>/2017/07/19/q-objects/</url>
      
        <content type="html"><![CDATA[<p>One of the great offers of Django is the high-level database API, which lets us manipulate our data using an object oriented approach.</p><p>For instance given the following model:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> django.db <span class="keyword">import</span> models</span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Grade</span>(<span class="params">models.Model</span>):</span></span><br><span class="line">    student_code = models.CharField(<span class="string">u&#x27;Student Code&#x27;</span>, max_length=<span class="number">30</span>)</span><br><span class="line">    course_code = models.CharField(<span class="string">u&#x27;Course code&#x27;</span>, max_length=<span class="number">30</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>What if we want to obtain all the grades for some combinations of student and courses. We might be tempted to use a *filter(**kwargs)* method with the <em>in</em> lookup:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> grades.models <span class="keyword">import</span> Grade</span><br><span class="line"> </span><br><span class="line">student_codes = [<span class="string">&#x27;456870&#x27;</span>,<span class="string">&#x27;433495&#x27;</span>,<span class="string">&#x27;104248&#x27;</span>,<span class="string">&#x27;720400&#x27;</span>]</span><br><span class="line">course_codes = [<span class="string">&#x27;474009&#x27;</span>,<span class="string">&#x27;431934&#x27;</span>,<span class="string">&#x27;447595&#x27;</span>,<span class="string">&#x27;789614&#x27;</span>]</span><br><span class="line">grades = Grade.objects.<span class="built_in">filter</span>(student_code__in=student_codes, </span><br><span class="line">                    course_codes__in=course_codes)</span><br></pre></td></tr></table></figure><p>However, this is not ideal since we will end up with grades from different combinations of student/course. This where Q objects come for the rescue. Q objects encapsulate conditions and can be merged together with different logical operators, such as OR and AND. Therefore, we could rewrite our previous approach as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> grades.models <span class="keyword">import</span> Grade</span><br><span class="line"><span class="keyword">from</span> django.db.models <span class="keyword">import</span> Q</span><br><span class="line"><span class="keyword">import</span> operator</span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> izip</span><br><span class="line"> </span><br><span class="line">student_codes = [<span class="string">&#x27;456870&#x27;</span>,<span class="string">&#x27;433495&#x27;</span>,<span class="string">&#x27;104248&#x27;</span>,<span class="string">&#x27;720400&#x27;</span>]</span><br><span class="line">course_codes = [<span class="string">&#x27;474009&#x27;</span>,<span class="string">&#x27;431934&#x27;</span>,<span class="string">&#x27;447595&#x27;</span>,<span class="string">&#x27;789614&#x27;</span>]</span><br><span class="line"> </span><br><span class="line">q_objects = (Q(student_code=sc, course_code=cc) <span class="keyword">for</span> sc, cc <span class="keyword">in</span> izip(student_codes, course_codes))</span><br><span class="line"> </span><br><span class="line">query = reduce(operator.or_, q_objects)</span><br><span class="line">grades = Grade.objects.<span class="built_in">filter</span>(query)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>The important part here is that we joining together a set of OR statements (our desired combinations) instead of allowing forbidden combinations that we are not interested. This example shows how we can overcome the limitations of <em>filter</em> methods by using Q objects, another great feature of our framework.</p>]]></content>
      
      
      
        <tags>
            
            <tag> django </tag>
            
            <tag> development </tag>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Problem solving techniques</title>
      <link href="/2017/07/16/problem-solving/"/>
      <url>/2017/07/16/problem-solving/</url>
      
        <content type="html"><![CDATA[<p>In our daily life, we are constantly faced with problems that we need to solve. Therefore, we should take this into account and be prepared.</p><p>Fortunatelly, there are many frameworks and systematic guidelines out there that we can follow. One of my favourites is based on the four principles outlined by <a href="https://en.wikipedia.org/wiki/George_P%C3%B3lya">George Polya</a> in 1945.</p><p><img src="/images/problem_solving.png" /></p><p><strong>1. Understanding the Problem</strong></p><p>In order to fully understand the problem, we may restate it in different words. We should describe all the availabe data and extract any possible information from it. The unknown variables and the conditions the data is subject to should also be noted.</p><p><strong>2. Devise a plan</strong></p><p>This step is probably the most important. Sometimes, different problems are related and maybe we can follow a strategy that we have used before. This recognition comes with practise and requires us to have knowledge of them. We may also try to solve an easier version of our problem and gradually add the other variables and conditions. Or we can try to recursively divide the big task into smaller sub-tasks and solve these ones first (divide and conquer). Even if we come up with different solutions, we should discuss the pros and cons of each in terms of different important factors.</p><p><strong>3. Carry out the plan</strong></p><p>In this step we just follow each outlined step of the plan. We should check that we are not forgetting anything and should take measures to guarantee that we are obtaining the expected results, for instance, logging and monitoring each iteration of our plan.</p><p><strong>4. Look back</strong></p><p>Finally, we confirm we did obtain the desired results, by interpreting them and asking if they make sense. In this step we may also document and share our strategy. This will help us in the future when similar problems appear and also help new people who are faced for the first time with this kind of challenges.</p>]]></content>
      
      
      
        <tags>
            
            <tag> problem-solving </tag>
            
            <tag> problems </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Getting started with Django - Building a simple calendar</title>
      <link href="/2017/07/15/django-calendar/"/>
      <url>/2017/07/15/django-calendar/</url>
      
        <content type="html"><![CDATA[<h2 id="what-is-django">What is Django ?</h2><p>Django is a well-known web framework, written in Python, which comes packaged with lots of out of the box features, such as: - An ORM (Object-relational mapping) where you define your data models and get access to a high-level API that lets you manage your data, instead of writing raw SQL. - A template system for writing the front end, extending the basic HTML functionality. - An admin panel where you can manage your data models. - Security measures. - Authentication, Form handling and URL routing.</p><p>This speeds up your development, since the most basic and common tasks are already provided to you and ready to be used, letting you focus on the logic that makes your product unique.</p><h2 id="getting-started">Getting started</h2><p><strong>Installing and activating a virtual environment</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ wget https://bootstrap.pypa.io/get-pip.py</span><br><span class="line">$ sudo python get-pip.py</span><br><span class="line">$ sudo pip install virtualenv</span><br><span class="line">$ virtualenv venv</span><br><span class="line">$ <span class="built_in">source</span> venv/bin/activate</span><br></pre></td></tr></table></figure><p><strong>Installing Django</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ pip install django==1.11.3</span><br></pre></td></tr></table></figure><p><strong>Creating the project directory structure</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ django-admin startproject mycalendar</span><br><span class="line">$ <span class="built_in">cd</span> mycalendar</span><br><span class="line">$ python manage.py startapp events</span><br></pre></td></tr></table></figure><p><strong>Aplplying the migrations, creating the admin account and running the web server</strong> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ python manage.py migrate</span><br><span class="line">$ python manage.py createsuperuser</span><br><span class="line">$ python manage.py runserver</span><br></pre></td></tr></table></figure></p><p>If you head over to http://localhost:8000/admin, you will see a simple administration interface.</p><h2 id="events">Events</h2><p>The basic unit of our calendar will be the event, so we start by writting our event model in <em>mycalendar/events/models.py</em>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> unicode_literals</span><br><span class="line"> </span><br><span class="line"><span class="keyword">from</span> django.db <span class="keyword">import</span> models</span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Event</span>(<span class="params">models.Model</span>):</span></span><br><span class="line">    day = models.DateField(<span class="string">u&#x27;Day of the event&#x27;</span>, help_text=<span class="string">u&#x27;Day of the event&#x27;</span>)</span><br><span class="line">    start_time = models.TimeField(<span class="string">u&#x27;Starting time&#x27;</span>, help_text=<span class="string">u&#x27;Starting time&#x27;</span>)</span><br><span class="line">    end_time = models.TimeField(<span class="string">u&#x27;Final time&#x27;</span>, help_text=<span class="string">u&#x27;Final time&#x27;</span>)</span><br><span class="line">    notes = models.TextField(<span class="string">u&#x27;Textual Notes&#x27;</span>, help_text=<span class="string">u&#x27;Textual Notes&#x27;</span>, blank=<span class="literal">True</span>, null=<span class="literal">True</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">Meta</span>:</span></span><br><span class="line">        verbose_name = <span class="string">u&#x27;Scheduling&#x27;</span></span><br><span class="line">        verbose_name_plural = <span class="string">u&#x27;Scheduling&#x27;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>After this step we need to tell django about our new app in <em>mycalendar/settings.py</em>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">(...)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># Application definition</span></span><br><span class="line"> </span><br><span class="line">INSTALLED_APPS = [</span><br><span class="line">    <span class="string">&#x27;django.contrib.admin&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;django.contrib.auth&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;django.contrib.contenttypes&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;django.contrib.sessions&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;django.contrib.messages&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;django.contrib.staticfiles&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;events&#x27;</span></span><br><span class="line">]</span><br><span class="line"> </span><br><span class="line">(...)</span><br></pre></td></tr></table></figure><p>And in <em>mycalendar/events/admin.py</em> we register the model. <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> unicode_literals</span><br><span class="line"> </span><br><span class="line"><span class="keyword">from</span> django.contrib <span class="keyword">import</span> admin</span><br><span class="line"><span class="keyword">from</span> models <span class="keyword">import</span> Event</span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EventAdmin</span>(<span class="params">admin.ModelAdmin</span>):</span></span><br><span class="line">    list_display = [<span class="string">&#x27;day&#x27;</span>, <span class="string">&#x27;start_time&#x27;</span>, <span class="string">&#x27;end_time&#x27;</span>, <span class="string">&#x27;notes&#x27;</span>]</span><br></pre></td></tr></table></figure></p><p>Finally, we apply the changes to the database and run the app: <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ python manage.py makemigrations</span><br><span class="line">$ python manage.py migrate</span><br><span class="line">$ python manage.py runserver</span><br></pre></td></tr></table></figure></p><p>We are now able to add simple events:</p><p><img src="/images/add_event.png" /></p><p>However, our current model does not have any validations, such as overlapping events. We can add these kind of validations in the <em>clean()</em> method of our model, which iterates over our events and checks if there are collisions.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> unicode_literals</span><br><span class="line"> </span><br><span class="line"><span class="keyword">from</span> django.db <span class="keyword">import</span> models</span><br><span class="line"><span class="keyword">from</span> django.core.exceptions <span class="keyword">import</span> ValidationError</span><br><span class="line"><span class="keyword">from</span> django.core.urlresolvers <span class="keyword">import</span> reverse</span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Event</span>(<span class="params">models.Model</span>):</span></span><br><span class="line">    day = models.DateField(<span class="string">u&#x27;Day of the event&#x27;</span>, help_text=<span class="string">u&#x27;Day of the event&#x27;</span>)</span><br><span class="line">    start_time = models.TimeField(<span class="string">u&#x27;Starting time&#x27;</span>, help_text=<span class="string">u&#x27;Starting time&#x27;</span>)</span><br><span class="line">    end_time = models.TimeField(<span class="string">u&#x27;Final time&#x27;</span>, help_text=<span class="string">u&#x27;Final time&#x27;</span>)</span><br><span class="line">    notes = models.TextField(<span class="string">u&#x27;Textual Notes&#x27;</span>, help_text=<span class="string">u&#x27;Textual Notes&#x27;</span>, blank=<span class="literal">True</span>, null=<span class="literal">True</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">Meta</span>:</span></span><br><span class="line">        verbose_name = <span class="string">u&#x27;Scheduling&#x27;</span></span><br><span class="line">        verbose_name_plural = <span class="string">u&#x27;Scheduling&#x27;</span></span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">check_overlap</span>(<span class="params">self, fixed_start, fixed_end, new_start, new_end</span>):</span></span><br><span class="line">        overlap = <span class="literal">False</span></span><br><span class="line">        <span class="keyword">if</span> new_start == fixed_end <span class="keyword">or</span> new_end == fixed_start:    <span class="comment">#edge case</span></span><br><span class="line">            overlap = <span class="literal">False</span></span><br><span class="line">        <span class="keyword">elif</span> (new_start &gt;= fixed_start <span class="keyword">and</span> new_start &lt;= fixed_end) <span class="keyword">or</span> (new_end &gt;= fixed_start <span class="keyword">and</span> new_end &lt;= fixed_end): <span class="comment">#innner limits</span></span><br><span class="line">            overlap = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">elif</span> new_start &lt;= fixed_start <span class="keyword">and</span> new_end &gt;= fixed_end: <span class="comment">#outter limits</span></span><br><span class="line">            overlap = <span class="literal">True</span></span><br><span class="line"> </span><br><span class="line">        <span class="keyword">return</span> overlap</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_absolute_url</span>(<span class="params">self</span>):</span></span><br><span class="line">        url = reverse(<span class="string">&#x27;admin:%s_%s_change&#x27;</span> % (self._meta.app_label, self._meta.model_name), args=[self.<span class="built_in">id</span>])</span><br><span class="line">        <span class="keyword">return</span> <span class="string">u&#x27;&lt;a href=&quot;%s&quot;&gt;%s&lt;/a&gt;&#x27;</span> % (url, <span class="built_in">str</span>(self.start_time))</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">clean</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">if</span> self.end_time &lt;= self.start_time:</span><br><span class="line">            <span class="keyword">raise</span> ValidationError(<span class="string">&#x27;Ending times must after starting times&#x27;</span>)</span><br><span class="line"> </span><br><span class="line">        events = Event.objects.<span class="built_in">filter</span>(day=self.day)</span><br><span class="line">        <span class="keyword">if</span> events.exists():</span><br><span class="line">            <span class="keyword">for</span> event <span class="keyword">in</span> events:</span><br><span class="line">                <span class="keyword">if</span> self.check_overlap(event.start_time, event.end_time, self.start_time, self.end_time):</span><br><span class="line">                    <span class="keyword">raise</span> ValidationError(</span><br><span class="line">                        <span class="string">&#x27;There is an overlap with another event: &#x27;</span> + <span class="built_in">str</span>(event.day) + <span class="string">&#x27;, &#x27;</span> + <span class="built_in">str</span>(</span><br><span class="line">                            event.start_time) + <span class="string">&#x27;-&#x27;</span> + <span class="built_in">str</span>(event.end_time))</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>Our app now detects collisions.</p><p><img src="/images/overlap.png" /></p><h2 id="calendar">Calendar</h2><p>It would be nice to add a monthly view of our events. There are some third-party packages available, but for the sake of simplicity we will stick to the built-in <em>HTMLCalendar</em> class provided by python.</p><p>We first need to override the <em>change_list.html</em> admin template by creating a file in <em>events/templates/admin/events/change_list.html</em> with the exactly the same content installed in <em>site-packages/django/contrib/admin/templates/admin/</em> .</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">(...)</span><br><span class="line">&#123;% block content %&#125;</span><br><span class="line">  <span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">&quot;content-main&quot;</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">ul</span> <span class="attr">class</span>=<span class="string">&quot;object-tools&quot;</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">li</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&#123;&#123;</span> <span class="attr">previous_month</span> &#125;&#125;&gt;</span></span><br><span class="line">                    Previous month</span><br><span class="line">                <span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">li</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&#123;&#123;</span> <span class="attr">next_month</span> &#125;&#125;&gt;</span></span><br><span class="line">                    Next month</span><br><span class="line">                <span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">ul</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        &#123;&#123; calendar &#125;&#125;</span><br><span class="line">(...)</span><br></pre></td></tr></table></figure><p>We will now dynamically define the context variables such as previous_month, next_month and calendar in <em>events/admin.py</em></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> unicode_literals</span><br><span class="line"> </span><br><span class="line"><span class="keyword">from</span> django.contrib <span class="keyword">import</span> admin</span><br><span class="line"><span class="keyword">from</span> models <span class="keyword">import</span> Event</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> calendar</span><br><span class="line"><span class="keyword">from</span> django.core.urlresolvers <span class="keyword">import</span> reverse</span><br><span class="line"><span class="keyword">from</span> calendar <span class="keyword">import</span> HTMLCalendar</span><br><span class="line"><span class="keyword">from</span> django.utils.safestring <span class="keyword">import</span> mark_safe</span><br><span class="line"> </span><br><span class="line"><span class="comment"># Register your models here.</span></span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EventAdmin</span>(<span class="params">admin.ModelAdmin</span>):</span></span><br><span class="line">    list_display = [<span class="string">&#x27;day&#x27;</span>, <span class="string">&#x27;start_time&#x27;</span>, <span class="string">&#x27;end_time&#x27;</span>, <span class="string">&#x27;notes&#x27;</span>]</span><br><span class="line">    change_list_template = <span class="string">&#x27;admin/events/change_list.html&#x27;</span></span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">changelist_view</span>(<span class="params">self, request, extra_context=<span class="literal">None</span></span>):</span></span><br><span class="line">        after_day = request.GET.get(<span class="string">&#x27;day__gte&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line">        extra_context = extra_context <span class="keyword">or</span> &#123;&#125;</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> after_day:</span><br><span class="line">            d = datetime.date.today()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                split_after_day = after_day.split(<span class="string">&#x27;-&#x27;</span>)</span><br><span class="line">                d = datetime.date(year=<span class="built_in">int</span>(split_after_day[<span class="number">0</span>]), month=<span class="built_in">int</span>(split_after_day[<span class="number">1</span>]), day=<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">except</span>:</span><br><span class="line">                d = datetime.date.today()</span><br><span class="line"> </span><br><span class="line">        previous_month = datetime.date(year=d.year, month=d.month, day=<span class="number">1</span>)  <span class="comment"># find first day of current month</span></span><br><span class="line">        previous_month = previous_month - datetime.timedelta(days=<span class="number">1</span>)  <span class="comment"># backs up a single day</span></span><br><span class="line">        previous_month = datetime.date(year=previous_month.year, month=previous_month.month,</span><br><span class="line">                                       day=<span class="number">1</span>)  <span class="comment"># find first day of previous month</span></span><br><span class="line"> </span><br><span class="line">        last_day = calendar.monthrange(d.year, d.month)</span><br><span class="line">        next_month = datetime.date(year=d.year, month=d.month, day=last_day[<span class="number">1</span>])  <span class="comment"># find last day of current month</span></span><br><span class="line">        next_month = next_month + datetime.timedelta(days=<span class="number">1</span>)  <span class="comment"># forward a single day</span></span><br><span class="line">        next_month = datetime.date(year=next_month.year, month=next_month.month,</span><br><span class="line">                                   day=<span class="number">1</span>)  <span class="comment"># find first day of next month</span></span><br><span class="line"> </span><br><span class="line">        extra_context[<span class="string">&#x27;previous_month&#x27;</span>] = reverse(<span class="string">&#x27;admin:events_event_changelist&#x27;</span>) + <span class="string">&#x27;?day__gte=&#x27;</span> + <span class="built_in">str</span>(</span><br><span class="line">            previous_month)</span><br><span class="line">        extra_context[<span class="string">&#x27;next_month&#x27;</span>] = reverse(<span class="string">&#x27;admin:events_event_changelist&#x27;</span>) + <span class="string">&#x27;?day__gte=&#x27;</span> + <span class="built_in">str</span>(next_month)</span><br><span class="line"> </span><br><span class="line">        cal = HTMLCalendar()</span><br><span class="line">        html_calendar = cal.formatmonth(d.year, d.month, withyear=<span class="literal">True</span>)</span><br><span class="line">        html_calendar = html_calendar.replace(<span class="string">&#x27;&lt;td &#x27;</span>, <span class="string">&#x27;&lt;td  width=&quot;150&quot; height=&quot;150&quot;&#x27;</span>)</span><br><span class="line">        extra_context[<span class="string">&#x27;calendar&#x27;</span>] = mark_safe(html_calendar)</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">super</span>(EventAdmin, self).changelist_view(request, extra_context)</span><br><span class="line"> </span><br><span class="line">admin.site.register(Event, EventAdmin)</span><br></pre></td></tr></table></figure><p>With this addition, our app now displays a monthly calendar.</p><p><img src="/images/calendar.png" /></p><p>However, our app still does not display the events in each cell. We can tweak the <em>HTMLCalendar</em> class by extending it and override the methods responsible for drawing the table cells. Our little tweak will be the introduction of the list of events and displaying them in their corresponding cell. We will now create a new class in <em>utils.py</em> .</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> calendar <span class="keyword">import</span> HTMLCalendar</span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime <span class="keyword">as</span> dtime, date, time</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">from</span> models <span class="keyword">import</span> Event</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EventCalendar</span>(<span class="params">HTMLCalendar</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, events=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(EventCalendar, self).__init__()</span><br><span class="line">        self.events = events</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">formatday</span>(<span class="params">self, day, weekday, events</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Return a day as a table cell.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        events_from_day = events.<span class="built_in">filter</span>(day__day=day)</span><br><span class="line">        events_html = <span class="string">&quot;&lt;ul&gt;&quot;</span></span><br><span class="line">        <span class="keyword">for</span> event <span class="keyword">in</span> events_from_day:</span><br><span class="line">            events_html += event.get_absolute_url() + <span class="string">&quot;&lt;br&gt;&quot;</span></span><br><span class="line">        events_html += <span class="string">&quot;&lt;/ul&gt;&quot;</span></span><br><span class="line"> </span><br><span class="line">        <span class="keyword">if</span> day == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">&#x27;&lt;td class=&quot;noday&quot;&gt;&amp;nbsp;&lt;/td&gt;&#x27;</span>  <span class="comment"># day outside month</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">&#x27;&lt;td class=&quot;%s&quot;&gt;%d%s&lt;/td&gt;&#x27;</span> % (self.cssclasses[weekday], day, events_html)</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">formatweek</span>(<span class="params">self, theweek, events</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Return a complete week as a table row.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        s = <span class="string">&#x27;&#x27;</span>.join(self.formatday(d, wd, events) <span class="keyword">for</span> (d, wd) <span class="keyword">in</span> theweek)</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;&lt;tr&gt;%s&lt;/tr&gt;&#x27;</span> % s</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">formatmonth</span>(<span class="params">self, theyear, themonth, withyear=<span class="literal">True</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Return a formatted month as a table.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"> </span><br><span class="line">        events = Event.objects.<span class="built_in">filter</span>(day__month=themonth)</span><br><span class="line"> </span><br><span class="line">        v = []</span><br><span class="line">        a = v.append</span><br><span class="line">        a(<span class="string">&#x27;&lt;table border=&quot;0&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; class=&quot;month&quot;&gt;&#x27;</span>)</span><br><span class="line">        a(<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">        a(self.formatmonthname(theyear, themonth, withyear=withyear))</span><br><span class="line">        a(<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">        a(self.formatweekheader())</span><br><span class="line">        a(<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> week <span class="keyword">in</span> self.monthdays2calendar(theyear, themonth):</span><br><span class="line">            a(self.formatweek(week, events))</span><br><span class="line">            a(<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">        a(<span class="string">&#x27;&lt;/table&gt;&#x27;</span>)</span><br><span class="line">        a(<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;&#x27;</span>.join(v)</span><br></pre></td></tr></table></figure><p>We can now use this new calendar in our admin view:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">(...)</span><br><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> EventCalendar</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">(...)</span><br><span class="line">cal = EventCalendar()</span><br><span class="line"> </span><br><span class="line">(...)</span><br></pre></td></tr></table></figure><p>Our monthly view now displays the events in their corresponding cells.</p><p><img src="/images/events.png" /></p><p>We can see a lot of events happening at 15th July. Fortunately, our new calendar also display links to their details.</p><p><img src="/images/birthday.png" /></p><p>Someone is having a very special day!</p><p>You can find the source code of this example <a href="https://github.com/AlexPnt/django-calendar">here</a>.</p>]]></content>
      
      
      
        <tags>
            
            <tag> django </tag>
            
            <tag> development </tag>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A set of core values</title>
      <link href="/2017/07/02/core-values/"/>
      <url>/2017/07/02/core-values/</url>
      
        <content type="html"><![CDATA[<p>In our daily lifes, we follow a set of principles that guide our behaviour and actions. They help us define and distinguish the right from wrong, deal with many situations, and follow what we believe to be the right path. Therefore, I like to keep a list of personal core values that I enjoy following.</p><ul><li>Listen.</li><li>Observe.</li><li>Exercise.</li><li>Experiment.<br /></li><li>Communicate.</li><li>Keep learning.</li><li>Enjoy moments.</li><li>Think and Ask.</li><li>Have a passion.</li><li>Avoid multitasking.</li><li>Be positive and motivated.</li><li>Do not ignore your intuiton.</li><li>Handle stressful situations calmy and with patience.</li><li>You cannot change the past, but you can affect the future.</li><li>Intercalate work with moments of leisure, instead of the opposite.</li><li>Things are built by increments. Focus on the small units and in time, you will achieve the bigger picture.</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> values </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A brave new blog</title>
      <link href="/2017/07/01/this-blog/"/>
      <url>/2017/07/01/this-blog/</url>
      
        <content type="html"><![CDATA[<p>I always enjoyed the idea of keeping a journal where ideas, tips and tricks from my daily life could be shared. With this first post I intend to move forward this goal. By sharing knowledge, you grow a bit and help others improve. It also helps you to keep motivated, giving you a sense of purpose since you feel that you are contributing to the community.</p><h2 id="the-hexo-framework">The Hexo Framework</h2><p>This site is built using the awesome <a href="https://hexo.io/">hexo framework</a> with a slightly customized version of the <a href="https://github.com/probberechts/cactus-dark">Cactus Dark</a> theme. In the light of the open source movement, you can find the <a href="https://github.com/AlexPnt/hexo-site">source code</a> of this site in my github account.</p><p>The <a href="https://hexo.io">hexo framework</a> is a fast and simple static site generator. Rather than generating dynamic content depending on the user request, a set of raw content is matched against a set of templates and styles to produce the final static content, that never changes over time. This has some advantages such as simplicity and speed, since there is no need to perform complex queries to complex backends to retrieve and build the content that the user is requesting.</p><h2 id="walkthrough">Walkthrough</h2><p>In order to get started you need to have the Node.js package manager (<a href="https://www.npmjs.com/">npm</a>) and the version control system <a href="https://git-scm.com/">git</a> installed.</p><p><strong>Setting up hexo:</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ npm install hexo-cli -g</span><br><span class="line">$ hexo init mysite</span><br><span class="line">$ <span class="built_in">cd</span> mysite</span><br><span class="line">$ npm install</span><br></pre></td></tr></table></figure><p><strong>Installing the cactus-dark theme:</strong> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ git <span class="built_in">clone</span> https://github.com/probberechts/cactus-dark.git themes/cactus-dark</span><br><span class="line">$ npm install hexo-pagination --save</span><br></pre></td></tr></table></figure></p><p>In your config.yml file, change the theme setting: <figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">theme:</span> <span class="string">cactus-dark</span></span><br></pre></td></tr></table></figure></p><p><strong>Start serving your new site:</strong> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br><span class="line">$ hexo serve</span><br></pre></td></tr></table></figure></p><p>Head over to http://localhost:4000 to see it in action! After this, go ahead and customize it to your needs. There is a basic folder structure that you have to keep in mind.</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">├── _config.yml - (configuration settings) </span><br><span class="line">├── public      - (static files, which may be deployed to a live server)</span><br><span class="line">├── scaffolds   - (pages and posts will be based on these scaffolds)</span><br><span class="line">├── source      - (raw source content of your site)</span><br><span class="line">└── themes      - (custom themes of your site)</span><br></pre></td></tr></table></figure><p><strong>Deploy:</strong></p><p>The last step is to deploy your generated static site to a remote server and start serving it. In my case, I simply push the public folder to my github repository.</p><p>For more details, head over to the great <a href="https://hexo.io/docs/">hexo documentation</a>.</p>]]></content>
      
      
      
        <tags>
            
            <tag> walkthrough </tag>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
